{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAv3fWJw3MCi"
      },
      "source": [
        "# LinkedIn Classification - Qwen2.5-3B QLoRA Fine-Tuning\n",
        "\n",
        "## Capstone Project: Julius-Maximilians-Universität Würzburg\n",
        "\n",
        "**Model:** Qwen2.5-3B-Instruct  \n",
        "**Method:** QLoRA (4-bit quantization + LoRA)  \n",
        "**Hardware:** Google Colab T4 (16GB VRAM)  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ7239cD3MCk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U -qq transformers>=4.44.0\n",
        "!pip install -U -qq peft>=0.12.0\n",
        "!pip install -U -qq trl>=0.9.0\n",
        "!pip install -U -qq bitsandbytes>=0.43.0\n",
        "!pip install -U -qq accelerate>=0.33.0\n",
        "!pip install -U -qq datasets scipy\n",
        "!pip install -qq pandas scikit-learn matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35dbxLCD3MCl"
      },
      "outputs": [],
      "source": [
        "import os, json, random, warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up5F8dQc3MCm"
      },
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n8GVc4v3MCm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TRAINING DATA - CSVs only\n",
        "df_sen_csv = pd.read_csv('seniority-v2.csv')[['text', 'label']].dropna()\n",
        "df_dept_csv = pd.read_csv('department-v2.csv')[['text', 'label']].dropna()\n",
        "\n",
        "# TEST DATA - 100% held out, NEVER train on this!\n",
        "with open('testdata.txt', 'r', encoding='utf-8') as f:\n",
        "    test_cvs = json.load(f)\n",
        "df_test = pd.DataFrame([j for cv in test_cvs for j in cv if j.get('status')=='ACTIVE'])\n",
        "\n",
        "# PRODUCTION DATA\n",
        "with open('more.txt', 'r', encoding='utf-8') as f:\n",
        "    more_cvs = json.load(f)\n",
        "df_prod = pd.DataFrame([j for cv in more_cvs for j in cv if j.get('status')=='ACTIVE'])\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"DATA SPLIT (ZERO LEAKAGE VERIFIED)\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"TRAINING (CSVs): {len(df_sen_csv)} seniority + {len(df_dept_csv)} department\")\n",
        "print(f\"TESTING (held out): {len(df_test)} samples\")\n",
        "print(f\"PRODUCTION: {len(df_prod)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42n9Ru-N3MCm"
      },
      "outputs": [],
      "source": [
        "# Labels\n",
        "SENIORITY_LABELS = ['Junior', 'Professional', 'Senior', 'Lead', 'Director', 'Management']\n",
        "DEPARTMENT_LABELS = ['Administrative', 'Business Development', 'Consulting', 'Customer Support',\n",
        "                     'Human Resources', 'Information Technology', 'Marketing', 'Other',\n",
        "                     'Project Management', 'Purchasing', 'Sales']\n",
        "\n",
        "# Estimation functions (for combining CSVs)\n",
        "def estimate_department(title):\n",
        "    if not title or pd.isna(title): return 'Other'\n",
        "    t = str(title).lower()\n",
        "    if any(x in t for x in ['software', 'developer', 'entwickler', 'engineer', 'it-', 'data', 'cto', 'devops']): return 'Information Technology'\n",
        "    if any(x in t for x in ['sales', 'vertrieb', 'account exec']): return 'Sales'\n",
        "    if any(x in t for x in ['marketing', 'brand', 'cmo', 'content']): return 'Marketing'\n",
        "    if any(x in t for x in ['hr', 'human', 'recruit', 'personal']): return 'Human Resources'\n",
        "    if any(x in t for x in ['consult', 'berat']): return 'Consulting'\n",
        "    if any(x in t for x in ['project', 'produkt', 'scrum', 'agile']): return 'Project Management'\n",
        "    if any(x in t for x in ['support', 'customer', 'service']): return 'Customer Support'\n",
        "    if any(x in t for x in ['admin', 'office', 'secretary']): return 'Administrative'\n",
        "    if any(x in t for x in ['purchas', 'einkauf', 'procurement']): return 'Purchasing'\n",
        "    if any(x in t for x in ['business develop', 'partner']): return 'Business Development'\n",
        "    return 'Other'\n",
        "\n",
        "def estimate_seniority(title):\n",
        "    if not title or pd.isna(title): return 'Professional'\n",
        "    t = str(title).lower()\n",
        "    if any(x in t for x in ['ceo', 'cfo', 'coo', 'cmo', 'cto', 'chief', 'geschäftsführer', 'founder', 'owner', 'inhaber', 'vorstand', 'prokurist']): return 'Management'\n",
        "    if 'director' in t and 'managing' not in t: return 'Director'\n",
        "    if any(x in t for x in ['head of', 'leiter', 'team lead', 'teamleiter', 'supervisor']): return 'Lead'\n",
        "    if any(x in t for x in ['senior', 'sr.', 'principal', 'staff', 'expert']): return 'Senior'\n",
        "    if any(x in t for x in ['junior', 'jr.', 'intern', 'trainee', 'praktikant', 'werkstudent', 'azubi']): return 'Junior'\n",
        "    return 'Professional'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOBhOwJO3MCn"
      },
      "outputs": [],
      "source": [
        "# Build training data from CSVs\n",
        "training_data = []\n",
        "\n",
        "# From seniority CSV (estimate department)\n",
        "for _, row in df_sen_csv.iterrows():\n",
        "    training_data.append({\n",
        "        'title': row['text'],\n",
        "        'seniority': row['label'],\n",
        "        'department': estimate_department(row['text'])\n",
        "    })\n",
        "\n",
        "# From department CSV (estimate seniority)\n",
        "for _, row in df_dept_csv.iterrows():\n",
        "    training_data.append({\n",
        "        'title': row['text'],\n",
        "        'seniority': estimate_seniority(row['text']),\n",
        "        'department': row['label']\n",
        "    })\n",
        "\n",
        "df_train = pd.DataFrame(training_data).drop_duplicates(subset=['title'])\n",
        "print(f\"Training samples: {len(df_train)}\")\n",
        "print(f\"\\nSeniority: {df_train['seniority'].value_counts().to_dict()}\")\n",
        "print(f\"Department: {df_train['department'].value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kn7gAtx3MCn"
      },
      "source": [
        "## 2. Create Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVOCJhPJ3MCn"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are an expert job title classifier. Classify into:\n",
        "\n",
        "SENIORITY:\n",
        "- Junior: Interns, trainees, entry-level (Praktikant, Werkstudent, Azubi)\n",
        "- Professional: Standard roles, default level (Engineer, Manager, Analyst)\n",
        "- Senior: Has \"Senior\", \"Sr.\", \"Principal\", \"Expert\" in title\n",
        "- Lead: Team leads, department heads (\"Head of\", \"Leiter\", \"Teamleiter\")\n",
        "- Director: Has \"Director\" for specific function (not Managing Director)\n",
        "- Management: C-level, owners, founders (CEO, CFO, Geschäftsführer, Inhaber)\n",
        "\n",
        "DEPARTMENT:\n",
        "- Information Technology: Software, IT, data, engineering, CTO\n",
        "- Sales: Sales reps, account executives\n",
        "- Marketing: Marketing, brand, communications\n",
        "- Human Resources: HR, recruiting\n",
        "- Consulting: Consultants, advisors\n",
        "- Project Management: Project/Product managers, Scrum masters\n",
        "- Business Development: Partnerships, strategic growth\n",
        "- Customer Support: Service, support\n",
        "- Administrative: Office, assistants\n",
        "- Purchasing: Procurement, supply chain\n",
        "- Other: Finance, legal, operations, executives (CEO, CFO)\n",
        "\n",
        "Respond ONLY:\n",
        "Seniority: [label]\n",
        "Department: [label]\"\"\"\n",
        "\n",
        "def create_prompt(title, seniority=None, department=None, for_training=True):\n",
        "    user_msg = f\"Classify this job title: {title}\"\n",
        "    if for_training:\n",
        "        assistant_msg = f\"Seniority: {seniority}\\nDepartment: {department}\"\n",
        "        return f\"\"\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{user_msg}<|im_end|>\\n<|im_start|>assistant\\n{assistant_msg}<|im_end|>\"\"\"\n",
        "    return f\"\"\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{user_msg}<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
        "\n",
        "# Create dataset\n",
        "train_prompts = [{'text': create_prompt(r['title'], r['seniority'], r['department'])} for _, r in df_train.iterrows()]\n",
        "train_dataset = Dataset.from_list(train_prompts)\n",
        "print(f\"Dataset: {len(train_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMtWTi0C3MCn"
      },
      "source": [
        "## 3. Load Qwen2.5-3B (4-bit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtQPQV1B3MCn"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.model_max_length = 512\n",
        "\n",
        "print(f\"Loading {MODEL_ID}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ii4cgeo3MCo"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    dtype=torch.float16,\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(f\"Loaded! VRAM: {torch.cuda.memory_allocated()/1e9:.1f}GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C65s6FG43MCo"
      },
      "source": [
        "## 4. Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVdbsRCl3MCo"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1gjTQvv3MCo"
      },
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkdsDdtc3MCo"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=\"./qwen_lora\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=200,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "def formatting_func(example):\n",
        "    return example[\"text\"]\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    args=sft_config,\n",
        "    formatting_func=formatting_func,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8fkhghZ3MCo"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "trainer.model.save_pretrained(\"./qwen_final\")\n",
        "tokenizer.save_pretrained(\"./qwen_final\")\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGEyYx8i3MCp"
      },
      "source": [
        "## 6. Evaluate on HELD-OUT Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT6rLuwj3MCp"
      },
      "outputs": [],
      "source": [
        "def predict(title):\n",
        "    prompt = create_prompt(title, for_training=False)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=50, temperature=0.1, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "    resp = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    resp = resp.split(\"assistant\")[-1].strip() if \"assistant\" in resp.lower() else resp\n",
        "\n",
        "    sen, dept = \"Professional\", \"Other\"\n",
        "    for line in resp.split(\"\\n\"):\n",
        "        if line.lower().startswith(\"seniority:\"):\n",
        "            s = line.split(\":\",1)[1].strip()\n",
        "            if s in SENIORITY_LABELS: sen = s\n",
        "        elif line.lower().startswith(\"department:\"):\n",
        "            d = line.split(\":\",1)[1].strip()\n",
        "            if d in DEPARTMENT_LABELS: dept = d\n",
        "    return sen, dept\n",
        "\n",
        "# Test\n",
        "print(predict(\"Senior Software Engineer\"))\n",
        "print(predict(\"Geschäftsführer\"))\n",
        "print(predict(\"Praktikant\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoagNM3y3MCp"
      },
      "outputs": [],
      "source": [
        "# Evaluate on HELD-OUT test data (testdata.txt - never seen during training!)\n",
        "print(\"Evaluating on HELD-OUT test data...\")\n",
        "preds = []\n",
        "for i, row in df_test.iterrows():\n",
        "    sen, dept = predict(row['position']) if pd.notna(row['position']) else (\"Professional\", \"Other\")\n",
        "    preds.append({'sen': sen, 'dept': dept})\n",
        "    if len(preds) % 25 == 0: print(f\"  {len(preds)}/{len(df_test)}\")\n",
        "\n",
        "sen_pred = [p['sen'] for p in preds]\n",
        "dept_pred = [p['dept'] for p in preds]\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6WTCPBc3MCp"
      },
      "outputs": [],
      "source": [
        "# Results\n",
        "sen_acc = accuracy_score(df_test['seniority'], sen_pred)\n",
        "dept_acc = accuracy_score(df_test['department'], dept_pred)\n",
        "sen_f1 = f1_score(df_test['seniority'], sen_pred, average='macro', zero_division=0)\n",
        "dept_f1 = f1_score(df_test['department'], dept_pred, average='macro', zero_division=0)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL RESULTS (on held-out testdata.txt)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Seniority:  {sen_acc*100:.1f}% acc, {sen_f1:.3f} F1\")\n",
        "print(f\"Department: {dept_acc*100:.1f}% acc, {dept_f1:.3f} F1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpHqaNqr3MCq"
      },
      "outputs": [],
      "source": [
        "print(\"\\nSENIORITY REPORT\")\n",
        "print(classification_report(df_test['seniority'], sen_pred, zero_division=0))\n",
        "print(\"\\nDEPARTMENT REPORT\")\n",
        "print(classification_report(df_test['department'], dept_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddRjUOov3MCq"
      },
      "outputs": [],
      "source": [
        "# Confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "for ax, (true, pred, title, cmap) in zip(axes, [\n",
        "    (df_test['seniority'], sen_pred, f'Seniority ({sen_acc*100:.1f}%)', 'Blues'),\n",
        "    (df_test['department'], dept_pred, f'Department ({dept_acc*100:.1f}%)', 'Oranges')]):\n",
        "    labels = sorted(set(true) | set(pred))\n",
        "    cm = confusion_matrix(true, pred, labels=labels)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, xticklabels=labels, yticklabels=labels, ax=ax)\n",
        "    ax.set_title(title, fontweight='bold')\n",
        "    ax.set_xlabel('Predicted'); ax.set_ylabel('Actual')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_qwen.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHkrtXdR3MCq"
      },
      "source": [
        "## 7. Production Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aeL3lNa3MCq"
      },
      "outputs": [],
      "source": [
        "print(\"Predicting on production data...\")\n",
        "prod_preds = []\n",
        "for i, row in df_prod.iterrows():\n",
        "    sen, dept = predict(row['position']) if pd.notna(row['position']) else (\"Professional\", \"Other\")\n",
        "    prod_preds.append({'sen': sen, 'dept': dept})\n",
        "    if len(prod_preds) % 50 == 0: print(f\"  {len(prod_preds)}/{len(df_prod)}\")\n",
        "\n",
        "df_prod['pred_seniority'] = [p['sen'] for p in prod_preds]\n",
        "df_prod['pred_department'] = [p['dept'] for p in prod_preds]\n",
        "df_prod[['position','organization','pred_department','pred_seniority']].to_csv('predictions_qwen.csv', index=False)\n",
        "print(f\"Saved {len(df_prod)} predictions!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl_y94Wk3MCq"
      },
      "outputs": [],
      "source": [
        "print(f\"\"\"\n",
        "{'='*60}\n",
        "SUMMARY\n",
        "{'='*60}\n",
        "Model: Qwen2.5-3B + QLoRA (4-bit)\n",
        "Training: {len(df_train)} samples (CSVs ONLY)\n",
        "Testing: {len(df_test)} samples (100% held-out testdata.txt)\n",
        "\n",
        "RESULTS:\n",
        "  Seniority:  {sen_acc*100:.1f}%\n",
        "  Department: {dept_acc*100:.1f}%\n",
        "\n",
        "Files: ./qwen_final/, predictions_qwen.csv\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}