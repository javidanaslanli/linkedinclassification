{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wTmQXUkajQQ"
      },
      "source": [
        "# LinkedIn Job Title Classification\n",
        "\n",
        "**Capstone Project** | Julius-Maximilians-Universität Würzburg\n",
        "\n",
        "---\n",
        "\n",
        "## Project Goal\n",
        "\n",
        "This notebook builds a **multi-task classification system** that takes LinkedIn job titles and predicts:\n",
        "1. **Seniority Level** - The career level of the position (6 classes)\n",
        "2. **Department** - The functional area of the position (11 classes)\n",
        "\n",
        "## Why This Matters\n",
        "\n",
        "Accurate job title classification enables:\n",
        "- **Recruiters**: Filter candidates by seniority and department\n",
        "- **HR Analytics**: Understand workforce composition\n",
        "- **Career Platforms**: Provide personalized job recommendations\n",
        "- **Market Research**: Analyze job market trends\n",
        "\n",
        "## Classification Categories\n",
        "\n",
        "| Seniority (6) | Department (11) |\n",
        "|---------------|------------------|\n",
        "| Junior | Information Technology |\n",
        "| Professional | Sales |\n",
        "| Senior | Marketing |\n",
        "| Lead | Human Resources |\n",
        "| Director | Consulting |\n",
        "| Management | Project Management |\n",
        "| | Business Development |\n",
        "| | Customer Support |\n",
        "| | Administrative |\n",
        "| | Purchasing |\n",
        "| | Other |\n",
        "\n",
        "## Notebook Sections\n",
        "\n",
        "| # | Section | Description |\n",
        "|---|---------|-------------|\n",
        "| 1 | Setup | Install packages, load configurations |\n",
        "| 2 | Data Loading | Load training CSVs and test JSON files |\n",
        "| 3 | EDA | Distribution analysis, language detection, train-test mismatch |\n",
        "| 4 | Feature Engineering | Text normalization, keyword indicators, career features |\n",
        "| 5 | Model Training | ConceptMoE (neural), LightGBM (boosting), LogReg (baseline) |\n",
        "| 6 | Ensemble + Fallback | Combine models with confidence-based keyword fallback |\n",
        "| 7 | Evaluation | Metrics, confusion matrices, error analysis |\n",
        "| 8 | Explainability | Feature importance, concept similarity analysis |\n",
        "| 9 | Demo | Interactive Gradio interface |\n",
        "| 10 | Export | Save trained models |\n",
        "| 11 | Improvements | Future work recommendations |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl7XYdkdajQU"
      },
      "source": [
        "---\n",
        "## Section 1: Setup and Configuration\n",
        "\n",
        "### Purpose\n",
        "This section prepares the environment by:\n",
        "1. Installing required Python packages\n",
        "2. Setting up paths for Google Colab or local execution\n",
        "3. Importing libraries and setting random seeds for reproducibility\n",
        "4. Loading external configuration files (keywords, concepts, training examples)\n",
        "\n",
        "### Why External Configs?\n",
        "We store keywords, concept definitions, and synthetic training examples in JSON files to:\n",
        "- Keep the notebook clean and readable\n",
        "- Allow easy updates without code changes\n",
        "- Enable configuration versioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XakxPuAKajQV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q sentence-transformers scikit-learn pandas numpy matplotlib seaborn torch lightgbm gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llGb8RYNajQW"
      },
      "source": [
        "### Mount Google Drive (Colab) or Set Local Path\n",
        "\n",
        "This cell automatically detects the execution environment:\n",
        "- **Google Colab**: Mounts your Google Drive and navigates to the project folder\n",
        "- **Local**: Uses the current directory or parent directory\n",
        "\n",
        "The `outputs/` and `models/` directories are created to store results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XPTquGmajQX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Try Google Drive first\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        BASE_PATH = '/content/drive/MyDrive/linkedin-classification'\n",
        "        if os.path.exists(BASE_PATH):\n",
        "            os.chdir(BASE_PATH)\n",
        "            print(f\"✓ Colab + Google Drive: {BASE_PATH}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"Drive folder not found\")\n",
        "    except:\n",
        "        # Fallback to Colab local files\n",
        "        BASE_PATH = '/content'\n",
        "        os.chdir(BASE_PATH)\n",
        "        print(f\"✓ Colab (local files): {BASE_PATH}\")\n",
        "        print(\"  Upload your data files using the file browser on the left\")\n",
        "else:\n",
        "    # Local Jupyter\n",
        "    BASE_PATH = '..' if os.path.basename(os.getcwd()) == 'notebooks' else '.'\n",
        "    os.chdir(BASE_PATH)\n",
        "    print(f\"✓ Local: {os.getcwd()}\")\n",
        "\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Show available files\n",
        "print(f\"\\nFiles in {os.getcwd()}:\")\n",
        "for f in os.listdir('.'):\n",
        "    if not f.startswith('.'):\n",
        "        print(f\"  - {f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4NLLQteajQX"
      },
      "source": [
        "### Import Libraries and Set Random Seeds\n",
        "\n",
        "We import all required libraries upfront for clarity. Random seeds are set for:\n",
        "- **NumPy**: Random operations in data processing\n",
        "- **PyTorch**: Neural network weight initialization and training\n",
        "- **Python random**: General randomization\n",
        "\n",
        "Using the same seed (42) ensures reproducible results across different runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8aamZ7iajQY"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,\n",
        "                             classification_report, confusion_matrix, balanced_accuracy_score)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import lightgbm as lgb\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {DEVICE}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (14, 5)\n",
        "plt.rcParams['font.size'] = 11\n",
        "sns.set_style('whitegrid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZcQLPk-ajQY"
      },
      "source": [
        "### Load External Configuration Files\n",
        "\n",
        "We load three JSON configuration files:\n",
        "\n",
        "1. **keywords.json**: Keyword rules for fallback classification when model confidence is low\n",
        "2. **concepts.json**: Semantic concept definitions used by ConceptMoE for similarity-based routing\n",
        "3. **training_titles.json**: Synthetic training examples to augment the original training data\n",
        "\n",
        "These files are located in the `config/` directory.\n",
        "\n",
        "\n",
        "You can copy the drive that we shared link to your own drive and mount it here :) if not you can load files manually to colab files, if you see any problem about loading please inform us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VujPEe3qajQZ"
      },
      "outputs": [],
      "source": [
        "# Set config path based on environment\n",
        "if IN_COLAB and BASE_PATH == '/content':\n",
        "    # Colab local files - no config folder\n",
        "    CONFIG_PATH = ''\n",
        "else:\n",
        "    # Google Drive or Local - use config folder\n",
        "    CONFIG_PATH = 'config/'\n",
        "\n",
        "with open(f'{CONFIG_PATH}keywords.json', 'r', encoding='utf-8') as f:\n",
        "    KEYWORDS_CONFIG = json.load(f)\n",
        "\n",
        "with open(f'{CONFIG_PATH}concepts.json', 'r', encoding='utf-8') as f:\n",
        "    CONCEPTS_CONFIG = json.load(f)\n",
        "\n",
        "with open(f'{CONFIG_PATH}training_titles.json', 'r', encoding='utf-8') as f:\n",
        "    TRAINING_CONFIG = json.load(f)\n",
        "\n",
        "SENIORITY_KEYWORDS = KEYWORDS_CONFIG['seniority']\n",
        "DEPARTMENT_KEYWORDS = KEYWORDS_CONFIG['department']\n",
        "SENIORITY_CONCEPTS = CONCEPTS_CONFIG['seniority']\n",
        "DEPARTMENT_CONCEPTS = CONCEPTS_CONFIG['department']\n",
        "\n",
        "print(\"✓ Configurations loaded successfully\")\n",
        "print(f\"  Config path: '{CONFIG_PATH}' (root)\" if CONFIG_PATH == '' else f\"  Config path: '{CONFIG_PATH}'\")\n",
        "print(f\"  Seniority classes: {list(SENIORITY_CONCEPTS.keys())}\")\n",
        "print(f\"  Department classes: {list(DEPARTMENT_CONCEPTS.keys())}\")\n",
        "print(f\"  Synthetic seniority examples: {sum(len(v) for v in TRAINING_CONFIG['seniority_titles'].values())}\")\n",
        "print(f\"  Synthetic department examples: {sum(len(v) for v in TRAINING_CONFIG['department_titles'].values())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO9LX5SBajQZ"
      },
      "source": [
        "---\n",
        "## Section 2: Data Loading\n",
        "\n",
        "### Purpose\n",
        "Load all data files needed for training and evaluation:\n",
        "\n",
        "### Data Sources\n",
        "1. **Training Data** (CSV files):\n",
        "   - `department-v2.csv`: Job titles with department labels\n",
        "   - `seniority-v2.csv`: Job titles with seniority labels\n",
        "\n",
        "2. **Test Data** (JSON files):\n",
        "   - `testdata.txt`: CV profiles with job history\n",
        "   - `more.txt`: Additional CV profiles (optional)\n",
        "\n",
        "### Test Data Structure\n",
        "Each CV profile contains multiple jobs. We extract only the **ACTIVE** (current) job for evaluation. Previous jobs are stored as career history for feature engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gArQP4TeajQa"
      },
      "outputs": [],
      "source": [
        "# Set data path based on environment\n",
        "if IN_COLAB and BASE_PATH == '/content':\n",
        "    # Colab local files - no data folder\n",
        "    DATA_PATH = ''\n",
        "else:\n",
        "    DATA_PATH = 'data/'\n",
        "\n",
        "df_dept_orig = pd.read_csv(f'{DATA_PATH}department-v2.csv')\n",
        "df_sen_orig = pd.read_csv(f'{DATA_PATH}seniority-v2.csv')\n",
        "\n",
        "with open(f'{DATA_PATH}testdata.txt', 'r', encoding='utf-8') as f:\n",
        "    test_cvs = json.load(f)\n",
        "\n",
        "try:\n",
        "    with open(f'{DATA_PATH}more.txt', 'r', encoding='utf-8') as f:\n",
        "        more_cvs = json.load(f)\n",
        "    print(f\"✓ Additional test data found: {len(more_cvs)} CVs\")\n",
        "except FileNotFoundError:\n",
        "    more_cvs = []\n",
        "\n",
        "print(f\"\\n✓ Data Loaded:\")\n",
        "print(f\"  Data path: '{DATA_PATH}' (root)\" if DATA_PATH == '' else f\"  Data path: '{DATA_PATH}'\")\n",
        "print(f\"  Department training: {len(df_dept_orig):,} samples\")\n",
        "print(f\"  Seniority training: {len(df_sen_orig):,} samples\")\n",
        "print(f\"  Test CVs: {len(test_cvs):,} profiles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHRSHqpUajQa"
      },
      "source": [
        "### Extract Active Jobs from CV Profiles\n",
        "\n",
        "Each CV profile is a list of jobs (positions held by one person). We:\n",
        "1. Find the **ACTIVE** job (current position) for evaluation\n",
        "2. Store **INACTIVE** jobs (previous positions) for career trajectory features\n",
        "3. Validate that labels exist for supervised evaluation\n",
        "\n",
        "This mimics real-world usage where we classify someone's current job title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEOuHU_AajQb"
      },
      "outputs": [],
      "source": [
        "def extract_active_jobs(cvs, require_labels=True):\n",
        "    results = []\n",
        "    stats = {'total': len(cvs), 'no_active': 0, 'no_labels': 0, 'valid': 0}\n",
        "\n",
        "    for cv in cvs:\n",
        "        active_job = None\n",
        "        inactive_jobs = []\n",
        "\n",
        "        for job in cv:\n",
        "            if job.get('status') == 'ACTIVE':\n",
        "                active_job = job\n",
        "            else:\n",
        "                inactive_jobs.append(job)\n",
        "\n",
        "        if active_job is None:\n",
        "            stats['no_active'] += 1\n",
        "            continue\n",
        "\n",
        "        if require_labels:\n",
        "            if not active_job.get('seniority') or pd.isna(active_job.get('seniority')):\n",
        "                stats['no_labels'] += 1\n",
        "                continue\n",
        "            if not active_job.get('department') or pd.isna(active_job.get('department')):\n",
        "                stats['no_labels'] += 1\n",
        "                continue\n",
        "\n",
        "        active_job['inactive_jobs'] = inactive_jobs\n",
        "        results.append(active_job)\n",
        "        stats['valid'] += 1\n",
        "\n",
        "    return pd.DataFrame(results), stats\n",
        "\n",
        "print(\"Extracting from testdata.txt:\")\n",
        "df_test, stats1 = extract_active_jobs(test_cvs)\n",
        "print(f\"  Total: {stats1['total']}, No active: {stats1['no_active']}, No labels: {stats1['no_labels']}, Valid: {stats1['valid']}\")\n",
        "\n",
        "if more_cvs:\n",
        "    print(\"\\nExtracting from more.txt:\")\n",
        "    df_more, stats2 = extract_active_jobs(more_cvs)\n",
        "    print(f\"  Total: {stats2['total']}, No active: {stats2['no_active']}, No labels: {stats2['no_labels']}, Valid: {stats2['valid']}\")\n",
        "    df_all_test = pd.concat([df_test, df_more], ignore_index=True)\n",
        "else:\n",
        "    df_all_test = df_test\n",
        "\n",
        "print(f\"\\nFinal test set: {len(df_all_test)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyze6Jd1ajQb"
      },
      "source": [
        "---\n",
        "## Section 3: Exploratory Data Analysis (EDA)\n",
        "\n",
        "### Purpose\n",
        "Understand the data before modeling to:\n",
        "1. **Identify class imbalance**: Some classes may have far more examples than others\n",
        "2. **Detect train-test mismatch**: Distribution differences between training and test data\n",
        "3. **Analyze language distribution**: Data is multilingual (EN, DE, FR)\n",
        "4. **Examine title patterns**: Length, structure, common words\n",
        "\n",
        "### Why This Matters\n",
        "- Class imbalance requires special handling (class weights, focal loss)\n",
        "- Distribution mismatch helps explain model errors\n",
        "- Language analysis ensures our multilingual model is appropriate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LouVimqaajQb"
      },
      "source": [
        "### 3.1 Training Data Class Distribution\n",
        "\n",
        "Visualize how training samples are distributed across classes. Look for:\n",
        "- **Dominant classes**: May cause model bias toward majority\n",
        "- **Underrepresented classes**: May have poor recall\n",
        "- **Imbalance ratio**: Difference between largest and smallest class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kQiQSkGajQb"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "sen_counts = df_sen_orig['label'].value_counts().sort_values()\n",
        "colors_sen = plt.cm.Blues(np.linspace(0.3, 0.9, len(sen_counts)))\n",
        "bars1 = axes[0].barh(sen_counts.index, sen_counts.values, color=colors_sen)\n",
        "axes[0].set_xlabel('Number of Samples')\n",
        "axes[0].set_title('Training Data: Seniority Distribution', fontsize=14, fontweight='bold')\n",
        "for i, (idx, val) in enumerate(sen_counts.items()):\n",
        "    axes[0].text(val + 50, i, f'{val:,} ({val/len(df_sen_orig)*100:.1f}%)', va='center', fontsize=10)\n",
        "\n",
        "dept_counts = df_dept_orig['label'].value_counts().sort_values()\n",
        "colors_dept = plt.cm.Oranges(np.linspace(0.3, 0.9, len(dept_counts)))\n",
        "bars2 = axes[1].barh(dept_counts.index, dept_counts.values, color=colors_dept)\n",
        "axes[1].set_xlabel('Number of Samples')\n",
        "axes[1].set_title('Training Data: Department Distribution', fontsize=14, fontweight='bold')\n",
        "for i, (idx, val) in enumerate(dept_counts.items()):\n",
        "    axes[1].text(val + 50, i, f'{val:,} ({val/len(df_dept_orig)*100:.1f}%)', va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTraining Data Statistics:\")\n",
        "print(f\"  Seniority - Most common: {sen_counts.idxmax()} ({sen_counts.max():,}), Least: {sen_counts.idxmin()} ({sen_counts.min():,})\")\n",
        "print(f\"  Seniority - Imbalance ratio: {sen_counts.max()/sen_counts.min():.1f}x\")\n",
        "print(f\"  Department - Most common: {dept_counts.idxmax()} ({dept_counts.max():,}), Least: {dept_counts.idxmin()} ({dept_counts.min():,})\")\n",
        "print(f\"  Department - Imbalance ratio: {dept_counts.max()/dept_counts.min():.1f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvxDujWhajQc"
      },
      "source": [
        "### 3.2 Test Data Class Distribution\n",
        "\n",
        "Compare test data distribution with training data. Differences here explain why certain classes may perform worse - the model wasn't trained on similar proportions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eSuAtGfajQc"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "test_sen = df_all_test['seniority'].value_counts().sort_values()\n",
        "colors_sen = plt.cm.Blues(np.linspace(0.3, 0.9, len(test_sen)))\n",
        "axes[0].barh(test_sen.index, test_sen.values, color=colors_sen)\n",
        "axes[0].set_xlabel('Number of Samples')\n",
        "axes[0].set_title('Test Data: Seniority Distribution', fontsize=14, fontweight='bold')\n",
        "for i, (idx, val) in enumerate(test_sen.items()):\n",
        "    axes[0].text(val + 1, i, f'{val} ({val/len(df_all_test)*100:.1f}%)', va='center', fontsize=10)\n",
        "\n",
        "test_dept = df_all_test['department'].value_counts().sort_values()\n",
        "colors_dept = plt.cm.Oranges(np.linspace(0.3, 0.9, len(test_dept)))\n",
        "axes[1].barh(test_dept.index, test_dept.values, color=colors_dept)\n",
        "axes[1].set_xlabel('Number of Samples')\n",
        "axes[1].set_title('Test Data: Department Distribution', fontsize=14, fontweight='bold')\n",
        "for i, (idx, val) in enumerate(test_dept.items()):\n",
        "    axes[1].text(val + 1, i, f'{val} ({val/len(df_all_test)*100:.1f}%)', va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T1WpqrpajQc"
      },
      "source": [
        "### 3.3 Train vs Test Distribution Mismatch Analysis\n",
        "\n",
        "Quantify the distribution difference between training and test data:\n",
        "- **Ratio < 0.33**: Class is 3x overrepresented in training\n",
        "- **Ratio > 3.0**: Class is 3x underrepresented in training\n",
        "\n",
        "High mismatch (especially for \"Other\" department) is a key challenge in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_s0ILJNajQc"
      },
      "outputs": [],
      "source": [
        "def analyze_distribution_mismatch(train_series, test_series, name):\n",
        "    train_dist = train_series.value_counts(normalize=True)\n",
        "    test_dist = test_series.value_counts(normalize=True)\n",
        "    all_classes = sorted(set(train_dist.index) | set(test_dist.index))\n",
        "\n",
        "    print(f\"\\n{'='*75}\")\n",
        "    print(f\"{name} DISTRIBUTION COMPARISON\")\n",
        "    print(f\"{'='*75}\")\n",
        "    print(f\"{'Class':<25} {'Train %':>10} {'Test %':>10} {'Ratio':>10} {'Status':>18}\")\n",
        "    print(\"-\" * 75)\n",
        "\n",
        "    warnings_list = []\n",
        "    for cls in all_classes:\n",
        "        train_pct = train_dist.get(cls, 0)\n",
        "        test_pct = test_dist.get(cls, 0)\n",
        "        ratio = test_pct / train_pct if train_pct > 0 else float('inf')\n",
        "\n",
        "        if ratio > 3 or ratio < 0.33:\n",
        "            status = \"⚠️  HIGH MISMATCH\"\n",
        "            warnings_list.append((cls, ratio))\n",
        "        elif ratio > 2 or ratio < 0.5:\n",
        "            status = \"⚡ Moderate\"\n",
        "        else:\n",
        "            status = \"✓  OK\"\n",
        "\n",
        "        print(f\"{cls:<25} {train_pct*100:>9.1f}% {test_pct*100:>9.1f}% {ratio:>9.2f}x {status:>18}\")\n",
        "\n",
        "    return warnings_list\n",
        "\n",
        "sen_warnings = analyze_distribution_mismatch(df_sen_orig['label'], df_all_test['seniority'], 'SENIORITY')\n",
        "dept_warnings = analyze_distribution_mismatch(df_dept_orig['label'], df_all_test['department'], 'DEPARTMENT')\n",
        "\n",
        "if sen_warnings or dept_warnings:\n",
        "    print(f\"\\n{'='*75}\")\n",
        "    print(\"⚠️  WARNING: Distribution mismatch detected\")\n",
        "    print(\"    The model uses class weighting and keyword fallback to handle this.\")\n",
        "    print(f\"{'='*75}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSAET1TJajQc"
      },
      "source": [
        "### 3.4 Language Distribution Analysis\n",
        "\n",
        "Our data is multilingual (English, German, French). We detect language based on:\n",
        "- Special characters (ä, ö, ü, ß for German; é, è, ç for French)\n",
        "- Language-specific keywords\n",
        "\n",
        "This informs our choice of a multilingual embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXOw_YzAajQd"
      },
      "outputs": [],
      "source": [
        "def detect_language(text):\n",
        "    if pd.isna(text) or not text:\n",
        "        return 'EN'\n",
        "    t = str(text).lower()\n",
        "    de_indicators = len(re.findall(r'[äöüß]', t)) * 2 + sum(1 for w in ['geschäftsführer', 'leiter', 'entwickler', 'berater', 'mitarbeiter'] if w in t) * 3\n",
        "    fr_indicators = len(re.findall(r'[éèêëàâùûîïôç]', t)) * 2 + sum(1 for w in ['directeur', 'responsable', 'chef', 'ingénieur'] if w in t) * 3\n",
        "\n",
        "    if de_indicators > fr_indicators and de_indicators > 0:\n",
        "        return 'DE'\n",
        "    if fr_indicators > 0:\n",
        "        return 'FR'\n",
        "    return 'EN'\n",
        "\n",
        "train_langs = df_sen_orig['text'].apply(detect_language).value_counts()\n",
        "test_langs = df_all_test['position'].apply(detect_language).value_counts()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "colors = {'EN': '#3498db', 'DE': '#e74c3c', 'FR': '#2ecc71'}\n",
        "\n",
        "axes[0].pie(train_langs.values, labels=train_langs.index, autopct='%1.1f%%',\n",
        "            colors=[colors.get(l, '#999') for l in train_langs.index], startangle=90)\n",
        "axes[0].set_title('Training Data: Language Distribution', fontweight='bold')\n",
        "\n",
        "axes[1].pie(test_langs.values, labels=test_langs.index, autopct='%1.1f%%',\n",
        "            colors=[colors.get(l, '#999') for l in test_langs.index], startangle=90)\n",
        "axes[1].set_title('Test Data: Language Distribution', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Language Distribution:\")\n",
        "print(f\"  Training: {dict(train_langs)}\")\n",
        "print(f\"  Test: {dict(test_langs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUGXoelwajQd"
      },
      "source": [
        "### 3.5 Job Title Length Analysis\n",
        "\n",
        "Analyze the word count distribution of job titles. This helps us understand:\n",
        "- Typical title length (most are 2-4 words)\n",
        "- Outliers (very long titles may contain additional info)\n",
        "- Differences between training and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bICNljruajQd"
      },
      "outputs": [],
      "source": [
        "train_lengths = df_sen_orig['text'].dropna().str.split().str.len()\n",
        "test_lengths = df_all_test['position'].dropna().str.split().str.len()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].hist(train_lengths, bins=range(1, 15), color='steelblue', alpha=0.7, edgecolor='white', rwidth=0.85)\n",
        "axes[0].axvline(train_lengths.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {train_lengths.mean():.1f}')\n",
        "axes[0].set_xlabel('Number of Words')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Training Data: Title Length Distribution', fontweight='bold')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].hist(test_lengths, bins=range(1, 15), color='darkorange', alpha=0.7, edgecolor='white', rwidth=0.85)\n",
        "axes[1].axvline(test_lengths.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {test_lengths.mean():.1f}')\n",
        "axes[1].set_xlabel('Number of Words')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Test Data: Title Length Distribution', fontweight='bold')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Title Length Statistics:\")\n",
        "print(f\"  Training - Mean: {train_lengths.mean():.2f}, Median: {train_lengths.median():.0f}, Max: {train_lengths.max()}\")\n",
        "print(f\"  Test - Mean: {test_lengths.mean():.2f}, Median: {test_lengths.median():.0f}, Max: {test_lengths.max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEe_wGkFajQd"
      },
      "source": [
        "### 3.6 Sample Data Preview\n",
        "\n",
        "Display random samples to get a feel for the data format and content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQoyiBGRajQd"
      },
      "outputs": [],
      "source": [
        "print(\"Sample Training Data (Seniority):\")\n",
        "print(df_sen_orig.sample(5, random_state=SEED)[['text', 'label']].to_string(index=False))\n",
        "\n",
        "print(\"\\nSample Training Data (Department):\")\n",
        "print(df_dept_orig.sample(5, random_state=SEED)[['text', 'label']].to_string(index=False))\n",
        "\n",
        "print(\"\\nSample Test Data:\")\n",
        "print(df_all_test[['position', 'organization', 'seniority', 'department']].sample(5, random_state=SEED).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZIWV7orajQd"
      },
      "source": [
        "---\n",
        "## Section 4: Feature Engineering\n",
        "\n",
        "### Purpose\n",
        "Transform raw job titles into numerical features for machine learning. We extract:\n",
        "\n",
        "1. **Text Normalization**: Standardize titles (lowercase, expand abbreviations)\n",
        "2. **Keyword Indicators**: Binary flags for domain keywords (has_senior, has_engineer, etc.)\n",
        "3. **Title Patterns**: Structural patterns (SENIOR_X, X_MANAGER, HEAD_OF_X)\n",
        "4. **Career Features**: Trajectory from job history (career going up, department consistency)\n",
        "\n",
        "### Why These Features?\n",
        "- **Keywords** capture explicit signals (\"Senior\" → Senior level)\n",
        "- **Patterns** capture title structure independent of specific words\n",
        "- **Career history** provides context (previous roles predict current level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b0PvhZYajQe"
      },
      "source": [
        "### 4.1 Text Normalization\n",
        "\n",
        "Standardize job titles for consistent feature extraction:\n",
        "- Expand abbreviations (Sr. → Senior, Jr. → Junior)\n",
        "- Remove special characters\n",
        "- Lowercase and normalize whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IChNWDXRajQe"
      },
      "outputs": [],
      "source": [
        "def normalize_title(title):\n",
        "    if pd.isna(title) or not title:\n",
        "        return \"\"\n",
        "    t = str(title).lower().strip()\n",
        "\n",
        "    abbreviations = {\n",
        "        r'\\bsr\\.?\\s': 'senior ',\n",
        "        r'\\bjr\\.?\\s': 'junior ',\n",
        "        r'\\bmgr\\.?\\b': 'manager',\n",
        "        r'\\beng\\.?\\b': 'engineer',\n",
        "        r'\\bdir\\.?\\b': 'director',\n",
        "        r'\\bvp\\b': 'vice president',\n",
        "    }\n",
        "    for pattern, replacement in abbreviations.items():\n",
        "        t = re.sub(pattern, replacement, t)\n",
        "\n",
        "    t = re.sub(r'[^\\w\\s]', ' ', t)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t\n",
        "\n",
        "examples = [\"Sr. Software Eng.\", \"Jr Developer\", \"VP Sales & Marketing\", \"Geschäftsführer (CEO)\"]\n",
        "print(\"Normalization Examples:\")\n",
        "for ex in examples:\n",
        "    print(f\"  '{ex}' → '{normalize_title(ex)}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF8gKZS6ajQe"
      },
      "source": [
        "### 4.2 Feature Extraction Functions\n",
        "\n",
        "Define functions to extract three types of features:\n",
        "\n",
        "1. **Title Features**: 23+ binary indicators for keywords + metadata (word count, char count)\n",
        "2. **Title Pattern**: Categorical pattern detection (SENIOR_X, HEAD_OF_X, etc.)\n",
        "3. **Career Features**: Statistics from job history (number of jobs, progression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccj1MsSrajQe"
      },
      "outputs": [],
      "source": [
        "SENIORITY_HIERARCHY = {'Junior': 1, 'Professional': 2, 'Senior': 3, 'Lead': 4, 'Director': 5, 'Management': 6}\n",
        "\n",
        "def detect_title_pattern(title):\n",
        "    t = normalize_title(title)\n",
        "    if 'head of' in t: return 'HEAD_OF_X'\n",
        "    if t.startswith('senior') or 'senior ' in t: return 'SENIOR_X'\n",
        "    if t.startswith('junior') or 'junior ' in t: return 'JUNIOR_X'\n",
        "    if any(c in t for c in ['chief', 'ceo', 'cfo', 'cto']): return 'C_LEVEL'\n",
        "    if 'director' in t: return 'DIRECTOR_X'\n",
        "    if 'manager' in t: return 'X_MANAGER'\n",
        "    if 'engineer' in t or 'developer' in t or 'entwickler' in t: return 'X_ENGINEER'\n",
        "    if 'analyst' in t: return 'X_ANALYST'\n",
        "    if 'consultant' in t or 'berater' in t: return 'CONSULTANT'\n",
        "    if any(w in t for w in ['intern', 'trainee', 'praktikant', 'werkstudent']): return 'INTERN'\n",
        "    return 'OTHER'\n",
        "\n",
        "def extract_title_features(title):\n",
        "    t = normalize_title(title)\n",
        "    return {\n",
        "        'title_words': len(t.split()) if t else 0,\n",
        "        'title_chars': len(t),\n",
        "        'has_senior': int('senior' in t),\n",
        "        'has_junior': int(any(w in t for w in ['junior', 'intern', 'trainee', 'praktikant', 'werkstudent', 'azubi'])),\n",
        "        'has_lead': int(any(w in t for w in ['lead', 'head of', 'leiter', 'teamleiter'])),\n",
        "        'has_director': int('director' in t or 'direktor' in t),\n",
        "        'has_c_level': int(any(w in t for w in ['chief', 'ceo', 'cfo', 'cto', 'geschäftsführer', 'vorstand'])),\n",
        "        'has_manager': int('manager' in t),\n",
        "        'has_engineer': int(any(w in t for w in ['engineer', 'developer', 'entwickler', 'programmer'])),\n",
        "        'has_analyst': int('analyst' in t),\n",
        "        'has_consultant': int('consultant' in t or 'berater' in t),\n",
        "        'has_sales': int(any(w in t for w in ['sales', 'vertrieb', 'account'])),\n",
        "        'has_marketing': int('marketing' in t),\n",
        "        'has_hr': int(any(w in t for w in ['human resources', 'hr ', 'recruiter', 'talent', 'personal'])),\n",
        "        'has_finance': int(any(w in t for w in ['finance', 'accountant', 'controller', 'finan'])),\n",
        "        'has_legal': int(any(w in t for w in ['lawyer', 'legal', 'attorney', 'rechtsanwalt'])),\n",
        "        'has_medical': int(any(w in t for w in ['doctor', 'nurse', 'physician', 'arzt'])),\n",
        "        'has_education': int(any(w in t for w in ['teacher', 'professor', 'lehrer'])),\n",
        "        'has_project': int('project' in t or 'projekt' in t),\n",
        "        'has_support': int(any(w in t for w in ['support', 'customer', 'service', 'kunde'])),\n",
        "        'has_admin': int(any(w in t for w in ['assistant', 'secretary', 'admin', 'office', 'büro'])),\n",
        "        'has_purchasing': int(any(w in t for w in ['purchasing', 'procurement', 'buyer', 'einkauf'])),\n",
        "        'has_bd': int('business development' in t),\n",
        "        'language': detect_language(title),\n",
        "        'title_pattern': detect_title_pattern(title),\n",
        "    }\n",
        "\n",
        "def infer_seniority_from_title(title):\n",
        "    t = normalize_title(title)\n",
        "    if any(w in t for w in ['chief', 'ceo', 'cfo', 'cto', 'founder', 'owner', 'geschäftsführer', 'vorstand', 'managing director', 'president']): return 'Management'\n",
        "    if 'director' in t and 'managing' not in t: return 'Director'\n",
        "    if any(w in t for w in ['head of', 'team lead', 'tech lead', 'leiter', 'teamleiter']): return 'Lead'\n",
        "    if any(w in t for w in ['senior', 'principal', 'staff']): return 'Senior'\n",
        "    if any(w in t for w in ['junior', 'intern', 'trainee', 'praktikant', 'werkstudent', 'azubi']): return 'Junior'\n",
        "    return 'Professional'\n",
        "\n",
        "def extract_career_features(inactive_jobs):\n",
        "    if not inactive_jobs:\n",
        "        return {\n",
        "            'num_previous_jobs': 0, 'career_going_up': 0.5, 'dept_consistent': 0.5,\n",
        "            'has_management_history': 0, 'has_lead_history': 0, 'has_senior_history': 0,\n",
        "            'max_seniority_level': 2, 'last_seniority_level': 2, 'avg_seniority_level': 2.0,\n",
        "            'career_progression_score': 0.5\n",
        "        }\n",
        "\n",
        "    seniorities = [infer_seniority_from_title(j.get('position', '')) for j in inactive_jobs]\n",
        "    levels = [SENIORITY_HIERARCHY.get(s, 2) for s in seniorities]\n",
        "\n",
        "    if len(levels) >= 2:\n",
        "        career_going_up = sum(1 for i in range(1, len(levels)) if levels[i] >= levels[i-1]) / (len(levels) - 1)\n",
        "        progression = (levels[-1] - levels[0]) / max(len(levels), 1)\n",
        "    else:\n",
        "        career_going_up = 0.5\n",
        "        progression = 0.0\n",
        "\n",
        "    return {\n",
        "        'num_previous_jobs': len(inactive_jobs),\n",
        "        'career_going_up': career_going_up,\n",
        "        'dept_consistent': 0.7,\n",
        "        'has_management_history': int('Management' in seniorities),\n",
        "        'has_lead_history': int(any(s in ['Lead', 'Director'] for s in seniorities)),\n",
        "        'has_senior_history': int('Senior' in seniorities),\n",
        "        'max_seniority_level': max(levels) if levels else 2,\n",
        "        'last_seniority_level': levels[-1] if levels else 2,\n",
        "        'avg_seniority_level': np.mean(levels) if levels else 2.0,\n",
        "        'career_progression_score': 0.5 + progression * 0.1,\n",
        "    }\n",
        "\n",
        "print(\"Feature extraction functions defined\")\n",
        "print(f\"  Title features: {len(extract_title_features('test'))} features\")\n",
        "print(f\"  Career features: {len(extract_career_features([]))} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JdYqzdYajQf"
      },
      "source": [
        "### 4.3 Prepare Training Data with Synthetic Examples\n",
        "\n",
        "Augment original training data with synthetic examples from `training_titles.json`.\n",
        "This helps:\n",
        "- Increase coverage of underrepresented classes\n",
        "- Provide multilingual examples\n",
        "- Add clear-cut examples to anchor the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0RceEeHajQf"
      },
      "outputs": [],
      "source": [
        "def create_synthetic_df(titles_dict):\n",
        "    data = []\n",
        "    for label, titles in titles_dict.items():\n",
        "        for title in titles:\n",
        "            data.append({'text': title, 'label': label})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "df_syn_sen = create_synthetic_df(TRAINING_CONFIG['seniority_titles'])\n",
        "df_syn_dept = create_synthetic_df(TRAINING_CONFIG['department_titles'])\n",
        "\n",
        "df_train_sen = pd.concat([df_sen_orig[['text', 'label']], df_syn_sen], ignore_index=True).drop_duplicates('text')\n",
        "df_train_dept = pd.concat([df_dept_orig[['text', 'label']], df_syn_dept], ignore_index=True).drop_duplicates('text')\n",
        "\n",
        "print(\"Combined Training Data:\")\n",
        "print(f\"  Seniority: {len(df_train_sen):,} (Original: {len(df_sen_orig):,} + Synthetic: {len(df_syn_sen)})\")\n",
        "print(f\"  Department: {len(df_train_dept):,} (Original: {len(df_dept_orig):,} + Synthetic: {len(df_syn_dept)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zcTzbSsajQf"
      },
      "source": [
        "### 4.4 Extract Features for Training Data\n",
        "\n",
        "Apply feature extraction to all training samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng3IYRxHajQf"
      },
      "outputs": [],
      "source": [
        "def process_training_data(df, text_col='text'):\n",
        "    features_list = []\n",
        "    for _, row in df.iterrows():\n",
        "        feat = extract_title_features(row[text_col])\n",
        "        feat.update(extract_career_features([]))\n",
        "        features_list.append(feat)\n",
        "    return pd.DataFrame(features_list)\n",
        "\n",
        "print(\"Extracting features for training data...\")\n",
        "\n",
        "df_train_sen_feat = process_training_data(df_train_sen)\n",
        "df_train_sen = pd.concat([df_train_sen.reset_index(drop=True), df_train_sen_feat], axis=1)\n",
        "print(f\"  Seniority training: done\")\n",
        "\n",
        "df_train_dept_feat = process_training_data(df_train_dept)\n",
        "df_train_dept = pd.concat([df_train_dept.reset_index(drop=True), df_train_dept_feat], axis=1)\n",
        "print(f\"  Department training: done\")\n",
        "\n",
        "print(f\"\\nTotal features extracted: {len(df_train_sen_feat.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTOxuUybajQg"
      },
      "source": [
        "### 4.5 Prepare Feature Matrices\n",
        "\n",
        "Convert feature DataFrames to numerical matrices for ML models:\n",
        "- Select numeric columns\n",
        "- One-hot encode categorical features (language, title_pattern)\n",
        "- Convert to float32 for PyTorch compatibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iexwPzAKajQg"
      },
      "outputs": [],
      "source": [
        "FEATURE_COLS = [\n",
        "    'title_words', 'title_chars',\n",
        "    'has_senior', 'has_junior', 'has_lead', 'has_director', 'has_c_level', 'has_manager',\n",
        "    'has_engineer', 'has_analyst', 'has_consultant', 'has_sales', 'has_marketing', 'has_hr',\n",
        "    'has_finance', 'has_legal', 'has_medical', 'has_education', 'has_project', 'has_support',\n",
        "    'has_admin', 'has_purchasing', 'has_bd',\n",
        "    'num_previous_jobs', 'career_going_up', 'dept_consistent',\n",
        "    'has_management_history', 'has_lead_history', 'has_senior_history',\n",
        "    'max_seniority_level', 'last_seniority_level', 'avg_seniority_level', 'career_progression_score'\n",
        "]\n",
        "\n",
        "def prepare_features(df, feature_cols):\n",
        "    features = df[feature_cols].copy()\n",
        "    lang_col = df['language'].iloc[:, 0] if isinstance(df['language'], pd.DataFrame) else df['language']\n",
        "    for lang in ['EN', 'DE', 'FR']:\n",
        "        features[f'lang_{lang}'] = (lang_col == lang).astype(int)\n",
        "    pattern_col = df['title_pattern'].iloc[:, 0] if isinstance(df['title_pattern'], pd.DataFrame) else df['title_pattern']\n",
        "    patterns = ['HEAD_OF_X', 'SENIOR_X', 'JUNIOR_X', 'C_LEVEL', 'DIRECTOR_X', 'X_MANAGER', 'X_ENGINEER', 'X_ANALYST', 'CONSULTANT', 'INTERN', 'OTHER']\n",
        "    for pat in patterns:\n",
        "        features[f'pattern_{pat}'] = (pattern_col == pat).astype(int)\n",
        "    return features.values.astype(np.float32)\n",
        "\n",
        "sen_train_feat = prepare_features(df_train_sen, FEATURE_COLS)\n",
        "dept_train_feat = prepare_features(df_train_dept, FEATURE_COLS)\n",
        "\n",
        "print(f\"Feature matrices prepared: {sen_train_feat.shape[1]} features per sample\")\n",
        "print(f\"  Seniority training: {sen_train_feat.shape}\")\n",
        "print(f\"  Department training: {dept_train_feat.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7YeR1NQajQg"
      },
      "source": [
        "---\n",
        "## Section 5: Model Training\n",
        "\n",
        "### Architecture Overview\n",
        "We train an **ensemble of three models** for each task:\n",
        "\n",
        "1. **ConceptMoE** (Neural Network)\n",
        "   - Uses sentence embeddings + concept similarity\n",
        "   - Mixture of Experts for complex patterns\n",
        "   - Handles semantic understanding\n",
        "\n",
        "2. **LightGBM** (Gradient Boosting)\n",
        "   - Uses engineered features only\n",
        "   - Captures explicit keyword rules\n",
        "   - Fast and interpretable\n",
        "\n",
        "3. **Logistic Regression** (Baseline)\n",
        "   - Uses embeddings + features\n",
        "   - Provides stable predictions\n",
        "   - Good calibration\n",
        "\n",
        "### Why an Ensemble?\n",
        "Each model has different strengths:\n",
        "- MoE captures semantic similarity (\"Software Developer\" ≈ \"Programmer\")\n",
        "- LightGBM captures explicit rules (\"Senior\" → Senior level)\n",
        "- LogReg provides stable baseline predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJd8JB-3ajQg"
      },
      "source": [
        "### 5.1 Load Embedding Model\n",
        "\n",
        "We use `multilingual-e5-large`, a state-of-the-art multilingual sentence encoder that:\n",
        "- Supports 100+ languages including English, German, French\n",
        "- Produces 1024-dimensional embeddings\n",
        "- Captures semantic meaning of job titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVTwnaLgajQn"
      },
      "outputs": [],
      "source": [
        "print(\"Loading embedding model (this may take a minute)...\")\n",
        "embed_model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "EMBED_DIM = embed_model.get_sentence_embedding_dimension()\n",
        "print(f\"Model loaded: multilingual-e5-large (dimension: {EMBED_DIM})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NawJt-hrajQn"
      },
      "source": [
        "### 5.2 Generate Embeddings\n",
        "\n",
        "Generate semantic embeddings for:\n",
        "1. **Concept texts**: Descriptions of each class (used for similarity routing)\n",
        "2. **Training titles**: All training job titles\n",
        "3. **Test titles**: All test job titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa0XhuYPajQn"
      },
      "outputs": [],
      "source": [
        "sen_concept_texts = [f\"{k}: {v['concept_text']}\" for k, v in SENIORITY_CONCEPTS.items()]\n",
        "dept_concept_texts = [f\"{k}: {v['concept_text']}\" for k, v in DEPARTMENT_CONCEPTS.items()]\n",
        "sen_labels = list(SENIORITY_CONCEPTS.keys())\n",
        "dept_labels = list(DEPARTMENT_CONCEPTS.keys())\n",
        "\n",
        "print(\"Encoding concepts...\")\n",
        "sen_concept_emb = embed_model.encode(sen_concept_texts, show_progress_bar=False)\n",
        "dept_concept_emb = embed_model.encode(dept_concept_texts, show_progress_bar=False)\n",
        "\n",
        "print(\"Encoding training data...\")\n",
        "sen_train_emb = embed_model.encode(df_train_sen['text'].tolist(), show_progress_bar=True, batch_size=64)\n",
        "dept_train_emb = embed_model.encode(df_train_dept['text'].tolist(), show_progress_bar=True, batch_size=64)\n",
        "\n",
        "print(\"Encoding test data...\")\n",
        "test_emb = embed_model.encode(df_all_test['position'].tolist(), show_progress_bar=True, batch_size=64)\n",
        "\n",
        "print(f\"\\nEmbeddings generated:\")\n",
        "print(f\"  Seniority training: {sen_train_emb.shape}\")\n",
        "print(f\"  Department training: {dept_train_emb.shape}\")\n",
        "print(f\"  Test: {test_emb.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S1rgjTBajQn"
      },
      "source": [
        "### 5.3 Encode Labels and Create Train/Val Split\n",
        "\n",
        "- Convert string labels to integers using LabelEncoder\n",
        "- Create concept matrices aligned with encoder classes\n",
        "- Split training data 90/10 for validation with stratification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0YPHL4WajQn"
      },
      "outputs": [],
      "source": [
        "sen_encoder = LabelEncoder()\n",
        "dept_encoder = LabelEncoder()\n",
        "sen_encoder.fit(list(SENIORITY_CONCEPTS.keys()))\n",
        "dept_encoder.fit(list(DEPARTMENT_CONCEPTS.keys()))\n",
        "\n",
        "sen_train_labels = sen_encoder.transform(df_train_sen['label'])\n",
        "dept_train_labels = dept_encoder.transform(df_train_dept['label'])\n",
        "\n",
        "sen_concept_matrix = np.stack([sen_concept_emb[sen_labels.index(l)] for l in sen_encoder.classes_])\n",
        "dept_concept_matrix = np.stack([dept_concept_emb[dept_labels.index(l)] for l in dept_encoder.classes_])\n",
        "\n",
        "X_sen_emb_tr, X_sen_emb_val, X_sen_feat_tr, X_sen_feat_val, y_sen_tr, y_sen_val = train_test_split(\n",
        "    sen_train_emb, sen_train_feat, sen_train_labels, test_size=0.1, random_state=SEED, stratify=sen_train_labels)\n",
        "\n",
        "X_dept_emb_tr, X_dept_emb_val, X_dept_feat_tr, X_dept_feat_val, y_dept_tr, y_dept_val = train_test_split(\n",
        "    dept_train_emb, dept_train_feat, dept_train_labels, test_size=0.1, random_state=SEED, stratify=dept_train_labels)\n",
        "\n",
        "print(f\"Train/Val split (90/10):\")\n",
        "print(f\"  Seniority: {len(X_sen_emb_tr):,} train / {len(X_sen_emb_val):,} val\")\n",
        "print(f\"  Department: {len(X_dept_emb_tr):,} train / {len(X_dept_emb_val):,} val\")\n",
        "print(f\"\\nLabel encoders:\")\n",
        "print(f\"  Seniority classes: {list(sen_encoder.classes_)}\")\n",
        "print(f\"  Department classes: {list(dept_encoder.classes_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAPHXDAuajQo"
      },
      "source": [
        "### 5.4 Define ConceptMoE Architecture\n",
        "\n",
        "**ConceptMoE** combines:\n",
        "1. **Cosine Similarity**: Computes similarity between input embedding and concept embeddings\n",
        "2. **Mixture of Experts**: Routes inputs to specialized expert networks\n",
        "3. **Focal Loss**: Handles class imbalance by focusing on hard examples\n",
        "\n",
        "The model learns to route different types of job titles to different experts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGBlJI0cajQo"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss\n",
        "\n",
        "class Expert(nn.Module):\n",
        "    def __init__(self, dim, hidden, dropout=0.15):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.Linear(dim, hidden), nn.SiLU(), nn.Dropout(dropout), nn.Linear(hidden, dim))\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class MoELayer(nn.Module):\n",
        "    def __init__(self, dim, num_experts, hidden_dim, top_k=2, dropout=0.15):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        self.gate = nn.Linear(dim, num_experts, bias=False)\n",
        "        self.experts = nn.ModuleList([Expert(dim, hidden_dim, dropout) for _ in range(num_experts)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate_logits = self.gate(x)\n",
        "        weights, indices = torch.topk(F.softmax(gate_logits, dim=-1), self.top_k)\n",
        "        weights = weights / weights.sum(dim=-1, keepdim=True)\n",
        "        output = torch.zeros_like(x)\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            mask = (indices == i).any(dim=-1)\n",
        "            if mask.any():\n",
        "                expert_out = expert(x[mask])\n",
        "                expert_weight = weights[mask, (indices[mask] == i).float().argmax(dim=-1)]\n",
        "                output[mask] += expert_out * expert_weight.unsqueeze(-1)\n",
        "        return output, gate_logits\n",
        "\n",
        "class ConceptMoE(nn.Module):\n",
        "    def __init__(self, embed_dim, num_classes, num_experts=4, hidden_dim=512, dropout=0.15):\n",
        "        super().__init__()\n",
        "        combined_dim = embed_dim + num_classes\n",
        "        self.input_proj = nn.Linear(combined_dim, hidden_dim)\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.moe = MoELayer(hidden_dim, num_experts, hidden_dim * 2, top_k=2, dropout=dropout)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, concept_matrix):\n",
        "        concept_sim = F.cosine_similarity(x.unsqueeze(1), concept_matrix.unsqueeze(0), dim=-1)\n",
        "        combined = torch.cat([x, concept_sim], dim=-1)\n",
        "        h = self.input_proj(combined)\n",
        "        h = self.norm1(h)\n",
        "        moe_out, gate_logits = self.moe(h)\n",
        "        h = h + self.dropout(moe_out)\n",
        "        h = self.norm2(h)\n",
        "        return self.classifier(h), gate_logits\n",
        "\n",
        "print(f\"ConceptMoE architecture defined\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in ConceptMoE(EMBED_DIM, 6).parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMSwlU2majQo"
      },
      "source": [
        "### 5.5 Train ConceptMoE Models\n",
        "\n",
        "Training configuration:\n",
        "- **Focal Loss**: Handles class imbalance by down-weighting easy examples\n",
        "- **Class Weights**: Computed automatically to balance classes\n",
        "- **Early Stopping**: Prevents overfitting by monitoring validation accuracy\n",
        "- **Cosine Annealing LR**: Gradually reduces learning rate\n",
        "- **Gradient Clipping**: Prevents exploding gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4u2v5qjajQo"
      },
      "outputs": [],
      "source": [
        "def train_moe(X_emb_tr, X_emb_val, y_tr, y_val, concept_matrix, num_classes, task_name, epochs=50, patience=8):\n",
        "    print(f\"\\nTraining ConceptMoE for {task_name}...\")\n",
        "\n",
        "    model = ConceptMoE(EMBED_DIM, num_classes, num_experts=4, hidden_dim=512, dropout=0.15).to(DEVICE)\n",
        "\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(y_tr), y=y_tr)\n",
        "    class_weights = torch.FloatTensor(class_weights).to(DEVICE)\n",
        "    criterion = FocalLoss(alpha=class_weights, gamma=2.0)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
        "\n",
        "    X_tr = torch.FloatTensor(X_emb_tr).to(DEVICE)\n",
        "    y_tr_t = torch.LongTensor(y_tr).to(DEVICE)\n",
        "    X_val = torch.FloatTensor(X_emb_val).to(DEVICE)\n",
        "    y_val_t = torch.LongTensor(y_val).to(DEVICE)\n",
        "    concept_t = torch.FloatTensor(concept_matrix).to(DEVICE)\n",
        "\n",
        "    dataset = TensorDataset(X_tr, y_tr_t)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    best_state = None\n",
        "    wait = 0\n",
        "    history = {'train_loss': [], 'val_acc': [], 'val_f1': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for bx, by in loader:\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(bx, concept_t)\n",
        "            loss = criterion(logits, by)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_logits, _ = model(X_val, concept_t)\n",
        "            val_preds = val_logits.argmax(dim=1).cpu().numpy()\n",
        "            val_acc = accuracy_score(y_val, val_preds)\n",
        "            val_f1 = f1_score(y_val, val_preds, average='macro', zero_division=0)\n",
        "\n",
        "        history['train_loss'].append(total_loss / len(loader))\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['val_f1'].append(val_f1)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"  Epoch {epoch+1}: Loss={total_loss/len(loader):.4f}, Acc={val_acc*100:.1f}%, F1={val_f1:.3f}\")\n",
        "\n",
        "        if wait >= patience:\n",
        "            print(f\"  Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    if best_state:\n",
        "        model.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})\n",
        "\n",
        "    print(f\"  Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
        "    return model, history\n",
        "\n",
        "moe_sen, history_sen = train_moe(X_sen_emb_tr, X_sen_emb_val, y_sen_tr, y_sen_val, sen_concept_matrix, len(sen_encoder.classes_), 'Seniority')\n",
        "moe_dept, history_dept = train_moe(X_dept_emb_tr, X_dept_emb_val, y_dept_tr, y_dept_val, dept_concept_matrix, len(dept_encoder.classes_), 'Department')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKccNM36ajQp"
      },
      "source": [
        "### 5.6 Training Curves\n",
        "\n",
        "Visualize training progress. Look for:\n",
        "- **Convergence**: Loss should decrease steadily\n",
        "- **No Overfitting**: Val metrics shouldn't degrade while train improves\n",
        "- **Early Stopping Point**: Where validation plateaued"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZraQ5yDajQp"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "axes[0, 0].plot(history_sen['train_loss'], label='Train Loss', color='steelblue')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].set_title('Seniority: Training Loss', fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "axes[0, 1].plot(history_sen['val_acc'], label='Accuracy', color='green')\n",
        "axes[0, 1].plot(history_sen['val_f1'], label='F1 Macro', color='orange')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Score')\n",
        "axes[0, 1].set_title('Seniority: Validation Metrics', fontweight='bold')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "axes[1, 0].plot(history_dept['train_loss'], label='Train Loss', color='darkorange')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Loss')\n",
        "axes[1, 0].set_title('Department: Training Loss', fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "axes[1, 1].plot(history_dept['val_acc'], label='Accuracy', color='green')\n",
        "axes[1, 1].plot(history_dept['val_f1'], label='F1 Macro', color='orange')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Score')\n",
        "axes[1, 1].set_title('Department: Validation Metrics', fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0mdbdO0ajQp"
      },
      "source": [
        "### 5.7 Train LightGBM Models\n",
        "\n",
        "LightGBM is a gradient boosting model that:\n",
        "- Works on engineered features only (no embeddings)\n",
        "- Captures explicit keyword rules effectively\n",
        "- Provides fast inference and interpretable feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKgj9c1xajQp"
      },
      "outputs": [],
      "source": [
        "def train_lgb(X_tr, X_val, y_tr, y_val, task_name):\n",
        "    print(f\"\\nTraining LightGBM for {task_name}...\")\n",
        "\n",
        "    model = lgb.LGBMClassifier(\n",
        "        n_estimators=300, learning_rate=0.05, max_depth=6, num_leaves=31,\n",
        "        class_weight='balanced', random_state=SEED, verbose=-1, n_jobs=-1\n",
        "    )\n",
        "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(30, verbose=False)])\n",
        "\n",
        "    val_preds = model.predict(X_val)\n",
        "    val_acc = accuracy_score(y_val, val_preds)\n",
        "    val_f1 = f1_score(y_val, val_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"  Validation: Acc={val_acc*100:.2f}%, F1={val_f1:.3f}\")\n",
        "    return model\n",
        "\n",
        "lgb_sen = train_lgb(X_sen_feat_tr, X_sen_feat_val, y_sen_tr, y_sen_val, 'Seniority')\n",
        "lgb_dept = train_lgb(X_dept_feat_tr, X_dept_feat_val, y_dept_tr, y_dept_val, 'Department')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz3i_bRYajQp"
      },
      "source": [
        "### 5.8 Train Logistic Regression Models\n",
        "\n",
        "Logistic Regression provides:\n",
        "- Stable baseline predictions\n",
        "- Good probability calibration\n",
        "- Works on combined embeddings + features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bog-WRCBajQq"
      },
      "outputs": [],
      "source": [
        "def train_lr(X_emb_tr, X_feat_tr, X_emb_val, X_feat_val, y_tr, y_val, task_name):\n",
        "    print(f\"\\nTraining Logistic Regression for {task_name}...\")\n",
        "\n",
        "    X_tr = np.hstack([X_emb_tr, X_feat_tr])\n",
        "    X_val = np.hstack([X_emb_val, X_feat_val])\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_tr_scaled = scaler.fit_transform(X_tr)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    model = LogisticRegression(max_iter=500, class_weight='balanced', random_state=SEED, n_jobs=-1)\n",
        "    model.fit(X_tr_scaled, y_tr)\n",
        "\n",
        "    val_preds = model.predict(X_val_scaled)\n",
        "    val_acc = accuracy_score(y_val, val_preds)\n",
        "    val_f1 = f1_score(y_val, val_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"  Validation: Acc={val_acc*100:.2f}%, F1={val_f1:.3f}\")\n",
        "    return model, scaler\n",
        "\n",
        "lr_sen, scaler_sen = train_lr(X_sen_emb_tr, X_sen_feat_tr, X_sen_emb_val, X_sen_feat_val, y_sen_tr, y_sen_val, 'Seniority')\n",
        "lr_dept, scaler_dept = train_lr(X_dept_emb_tr, X_dept_feat_tr, X_dept_emb_val, X_dept_feat_val, y_dept_tr, y_dept_val, 'Department')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snK7d_B2ajQq"
      },
      "source": [
        "---\n",
        "## Section 6: Ensemble with Keyword Fallback\n",
        "\n",
        "### Ensemble Strategy\n",
        "Combine predictions using weighted averaging:\n",
        "\n",
        "**Seniority Weights**: MoE (50%) + LightGBM (40%) + LogReg (10%)\n",
        "- Higher MoE weight: semantic understanding matters for seniority\n",
        "\n",
        "**Department Weights**: MoE (30%) + LightGBM (40%) + LogReg (30%)\n",
        "- Higher LightGBM weight: explicit keywords matter more for department\n",
        "\n",
        "### Keyword Fallback\n",
        "When ensemble confidence is low (< threshold), use keyword-based rules:\n",
        "- Explicit keywords like \"CEO\", \"Senior\", \"Intern\" are reliable signals\n",
        "- Fallback only triggers when the model is uncertain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGbtwTvKajQq"
      },
      "source": [
        "### 6.1 Keyword Classification Functions\n",
        "\n",
        "Rule-based classifiers using keyword matching. Each function returns:\n",
        "- `prediction`: The predicted class\n",
        "- `confidence`: How confident the rule is (0.0 - 1.0)\n",
        "\n",
        "Higher confidence means more specific keyword matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvGZg1iyajQq"
      },
      "outputs": [],
      "source": [
        "def keyword_seniority_classification(title: str) -> Tuple[str, float]:\n",
        "    if pd.isna(title):\n",
        "        return 'Professional', 0.5\n",
        "\n",
        "    t = normalize_title(title)\n",
        "\n",
        "    management_kw = [\n",
        "        'ceo', 'cfo', 'cto', 'coo', 'cmo', 'cio', 'chro', 'cpo', 'cro',\n",
        "        'chief executive', 'chief financial', 'chief technology', 'chief operating',\n",
        "        'chief marketing', 'chief information', 'chief revenue', 'chief product',\n",
        "        'founder', 'co founder', 'cofounder', 'owner', 'partner', 'shareholder',\n",
        "        'managing director', 'general manager', 'president', 'vice president',\n",
        "        'vp ', 'svp ', 'evp ', 'avp ', 'board member', 'board of director',\n",
        "        'geschäftsführer', 'geschäftsführerin', 'vorstand', 'vorstandsvorsitzender',\n",
        "        'inhaber', 'inhaberin', 'eigentümer', 'gesellschafter', 'prokurist',\n",
        "        'pdg', 'président', 'présidente', 'directeur général', 'gérant', 'fondateur',\n",
        "        'advisory board', 'supervisory board', 'aufsichtsrat'\n",
        "    ]\n",
        "    if any(kw in t for kw in management_kw):\n",
        "        return 'Management', 0.95\n",
        "\n",
        "    director_kw = [\n",
        "        'director of', 'director,', ' director ', 'sales director', 'marketing director',\n",
        "        'finance director', 'hr director', 'technical director', 'creative director',\n",
        "        'operations director', 'it director', 'regional director', 'country director',\n",
        "        'direktor', 'direktorin', 'directeur', 'directrice'\n",
        "    ]\n",
        "    if any(kw in t for kw in director_kw) and 'managing director' not in t:\n",
        "        return 'Director', 0.90\n",
        "\n",
        "    lead_kw = [\n",
        "        'head of', 'head,', ' head ', 'team lead', 'tech lead', 'technical lead',\n",
        "        'engineering lead', 'lead engineer', 'lead developer', 'lead designer',\n",
        "        'lead analyst', 'lead architect', 'lead consultant', 'lead scientist',\n",
        "        'team leader', 'group leader', 'squad leader', 'chapter lead',\n",
        "        'leiter', 'leiterin', 'leitung', 'abteilungsleiter', 'bereichsleiter',\n",
        "        'teamleiter', 'gruppenleiter', 'projektleiter',\n",
        "        'chef de', 'responsable', 'superviseur', 'supervisor'\n",
        "    ]\n",
        "    if any(kw in t for kw in lead_kw):\n",
        "        return 'Lead', 0.90\n",
        "\n",
        "    senior_kw = [\n",
        "        'senior ', 'senior,', 'principal', 'staff engineer', 'staff developer',\n",
        "        'staff scientist', 'distinguished', 'fellow', 'architect',\n",
        "        'expert', 'experte', 'expertin', 'fachexperte', 'specialist', 'spezialist'\n",
        "    ]\n",
        "    if any(kw in t for kw in senior_kw):\n",
        "        return 'Senior', 0.90\n",
        "\n",
        "    junior_kw = [\n",
        "        'junior ', 'junior,', 'intern', 'internship', 'trainee', 'apprentice',\n",
        "        'graduate', 'entry level', 'entry-level', 'associate ',\n",
        "        'praktikant', 'praktikantin', 'werkstudent', 'werkstudentin',\n",
        "        'azubi', 'auszubildend', 'volontär', 'volontärin',\n",
        "        'stagiaire', 'stage', 'alternant', 'alternante', 'apprenti', 'débutant'\n",
        "    ]\n",
        "    if any(kw in t for kw in junior_kw):\n",
        "        return 'Junior', 0.95\n",
        "\n",
        "    return 'Professional', 0.75\n",
        "\n",
        "\n",
        "def keyword_department_classification(title: str, organization: str = None) -> Tuple[str, float]:\n",
        "    if pd.isna(title):\n",
        "        return 'Other', 0.5\n",
        "\n",
        "    t = normalize_title(title)\n",
        "    org = normalize_title(organization) if organization else \"\"\n",
        "\n",
        "    it_kw = ['software', 'developer', 'engineer', 'programmer', 'coder', 'data scientist',\n",
        "             'data engineer', 'data analyst', 'machine learning', 'devops', 'backend',\n",
        "             'frontend', 'full stack', 'fullstack', 'cloud', 'aws', 'azure', 'kubernetes',\n",
        "             'docker', 'database', 'sql', 'python', 'java', 'javascript', 'web developer',\n",
        "             'mobile developer', 'ios developer', 'android developer', 'qa engineer',\n",
        "             'test engineer', 'security engineer', 'it manager', 'it director', 'it support',\n",
        "             'system admin', 'entwickler', 'softwareentwickler', 'programmierer', 'informatik',\n",
        "             'développeur', 'ingénieur informatique']\n",
        "    if any(kw in t for kw in it_kw):\n",
        "        return 'Information Technology', 0.90\n",
        "\n",
        "    sales_kw = ['sales', 'account executive', 'account manager', 'key account',\n",
        "                'vertrieb', 'verkauf', 'außendienst', 'innendienst', 'commercial',\n",
        "                'territory manager', 'regional sales', 'business representative', 'client manager']\n",
        "    if any(kw in t for kw in sales_kw) and 'presales' not in t:\n",
        "        return 'Sales', 0.90\n",
        "\n",
        "    marketing_kw = ['marketing', 'brand manager', 'brand director', 'seo', 'sem',\n",
        "                    'social media', 'content manager', 'content strategist', 'digital marketing',\n",
        "                    'performance marketing', 'growth marketing', 'communications',\n",
        "                    'public relations', 'pr manager', 'campaign manager', 'marketing analyst']\n",
        "    if any(kw in t for kw in marketing_kw):\n",
        "        return 'Marketing', 0.90\n",
        "\n",
        "    hr_kw = ['human resources', ' hr ', 'hr manager', 'hr director', 'hr business partner',\n",
        "             'recruiter', 'recruiting', 'talent acquisition', 'talent manager',\n",
        "             'people operations', 'people manager', 'compensation', 'benefits',\n",
        "             'learning development', 'training manager', 'personal', 'personalleiter',\n",
        "             'personalsachbearbeiter', 'ressources humaines', 'rh ']\n",
        "    if any(kw in t for kw in hr_kw):\n",
        "        return 'Human Resources', 0.90\n",
        "\n",
        "    consulting_kw = ['consultant', 'consulting', 'advisory', 'advisor', 'management consultant',\n",
        "                     'strategy consultant', 'it consultant', 'berater', 'beratung',\n",
        "                     'unternehmensberater', 'conseil', 'conseiller']\n",
        "    consulting_orgs = ['mckinsey', 'bain', 'bcg', 'deloitte', 'pwc', 'kpmg', 'ey', 'accenture', 'capgemini']\n",
        "    if any(kw in t for kw in consulting_kw) or any(c in org for c in consulting_orgs):\n",
        "        return 'Consulting', 0.85\n",
        "\n",
        "    pm_kw = ['project manager', 'program manager', 'portfolio manager', 'product manager',\n",
        "             'product owner', 'scrum master', 'agile coach', 'delivery manager',\n",
        "             'release manager', 'pmo', 'projektmanager', 'projektleiter', 'produktmanager',\n",
        "             'chef de projet', 'responsable produit']\n",
        "    if any(kw in t for kw in pm_kw):\n",
        "        return 'Project Management', 0.90\n",
        "\n",
        "    bd_kw = ['business development', 'bd manager', 'biz dev', 'partnership', 'alliance',\n",
        "             'channel manager', 'expansion', 'growth manager', 'market development']\n",
        "    if any(kw in t for kw in bd_kw):\n",
        "        return 'Business Development', 0.85\n",
        "\n",
        "    support_kw = ['customer service', 'customer support', 'customer success', 'technical support',\n",
        "                  'help desk', 'service desk', 'client services', 'client support',\n",
        "                  'kundenberater', 'kundenbetreuer', 'support']\n",
        "    if any(kw in t for kw in support_kw):\n",
        "        return 'Customer Support', 0.85\n",
        "\n",
        "    admin_kw = ['office manager', 'executive assistant', 'personal assistant',\n",
        "                'administrative assistant', 'secretary', 'receptionist', 'office administrator',\n",
        "                'operations coordinator', 'bürokaufmann', 'sekretär', 'sekretärin',\n",
        "                'assistentin', 'assistant de direction', 'secrétaire']\n",
        "    if any(kw in t for kw in admin_kw):\n",
        "        return 'Administrative', 0.85\n",
        "\n",
        "    purchasing_kw = ['purchasing', 'procurement', 'buyer', 'sourcing', 'supply chain',\n",
        "                     'vendor manager', 'category manager', 'einkäufer', 'einkauf',\n",
        "                     'beschaffung', 'acheteur', 'achats']\n",
        "    if any(kw in t for kw in purchasing_kw):\n",
        "        return 'Purchasing', 0.85\n",
        "\n",
        "    return 'Other', 0.60\n",
        "\n",
        "print(\"Keyword classification functions defined\")\n",
        "print(\"\\nTest examples:\")\n",
        "for title in [\"CEO\", \"Senior Engineer\", \"Intern\", \"Sales Manager\", \"Freelancer\"]:\n",
        "    sen, sen_c = keyword_seniority_classification(title)\n",
        "    dept, dept_c = keyword_department_classification(title, \"\")\n",
        "    print(f\"  '{title}' → Seniority: {sen} ({sen_c:.0%}), Department: {dept} ({dept_c:.0%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ6tXRM-ajQr"
      },
      "source": [
        "### 6.2 Ensemble Prediction Functions\n",
        "\n",
        "Combine all three models with keyword fallback:\n",
        "1. Get probability predictions from each model\n",
        "2. Compute weighted average\n",
        "3. Calculate entropy-based confidence\n",
        "4. If confidence < threshold, try keyword fallback\n",
        "5. Use fallback only if keyword confidence > ensemble confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCwRPkURajQr"
      },
      "outputs": [],
      "source": [
        "def entropy_confidence(probs):\n",
        "    entropy = -np.sum(probs * np.log(probs + 1e-10), axis=-1)\n",
        "    max_entropy = np.log(probs.shape[-1])\n",
        "    return 1 - (entropy / max_entropy)\n",
        "\n",
        "def ensemble_predict_seniority(titles, embeddings, features, moe_model, lgb_model, lr_model, scaler, encoder,\n",
        "                               weights=[0.50, 0.40, 0.10], confidence_threshold=0.5):\n",
        "    moe_model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_t = torch.FloatTensor(embeddings).to(DEVICE)\n",
        "        concept_t = torch.FloatTensor(sen_concept_matrix).to(DEVICE)\n",
        "        moe_out = moe_model(X_t, concept_t)\n",
        "        moe_logits = moe_out[0] if isinstance(moe_out, tuple) else moe_out\n",
        "        moe_probs = F.softmax(moe_logits, dim=-1).cpu().numpy()\n",
        "\n",
        "    lgb_probs = lgb_model.predict_proba(features)\n",
        "    X_lr = np.concatenate([embeddings, features], axis=1)\n",
        "    X_lr_scaled = scaler.transform(X_lr)\n",
        "    lr_probs = lr_model.predict_proba(X_lr_scaled)\n",
        "\n",
        "    combined_probs = weights[0] * moe_probs + weights[1] * lgb_probs + weights[2] * lr_probs\n",
        "    ens_preds = encoder.classes_[np.argmax(combined_probs, axis=1)]\n",
        "    ens_confs = entropy_confidence(combined_probs)\n",
        "\n",
        "    final_preds = []\n",
        "    fallback_count = 0\n",
        "\n",
        "    for title, ens_pred, ens_conf in zip(titles, ens_preds, ens_confs):\n",
        "        if ens_conf < confidence_threshold:\n",
        "            kw_pred, kw_conf = keyword_seniority_classification(title)\n",
        "            if kw_conf > ens_conf:\n",
        "                final_preds.append(kw_pred)\n",
        "                fallback_count += 1\n",
        "            else:\n",
        "                final_preds.append(ens_pred)\n",
        "        else:\n",
        "            final_preds.append(ens_pred)\n",
        "\n",
        "    print(f\"  Seniority fallback: {fallback_count}/{len(titles)} ({fallback_count/len(titles)*100:.1f}%)\")\n",
        "    return np.array(final_preds), ens_confs, combined_probs\n",
        "\n",
        "\n",
        "def ensemble_predict_department(titles, organizations, embeddings, features, moe_model, lgb_model, lr_model, scaler, encoder,\n",
        "                                weights=[0.30, 0.40, 0.30], confidence_threshold=0.5):\n",
        "    moe_model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_t = torch.FloatTensor(embeddings).to(DEVICE)\n",
        "        concept_t = torch.FloatTensor(dept_concept_matrix).to(DEVICE)\n",
        "        moe_out = moe_model(X_t, concept_t)\n",
        "        moe_logits = moe_out[0] if isinstance(moe_out, tuple) else moe_out\n",
        "        moe_probs = F.softmax(moe_logits, dim=-1).cpu().numpy()\n",
        "\n",
        "    lgb_probs = lgb_model.predict_proba(features)\n",
        "    X_lr = np.concatenate([embeddings, features], axis=1)\n",
        "    X_lr_scaled = scaler.transform(X_lr)\n",
        "    lr_probs = lr_model.predict_proba(X_lr_scaled)\n",
        "\n",
        "    combined_probs = weights[0] * moe_probs + weights[1] * lgb_probs + weights[2] * lr_probs\n",
        "    ens_preds = encoder.classes_[np.argmax(combined_probs, axis=1)]\n",
        "    ens_confs = entropy_confidence(combined_probs)\n",
        "\n",
        "    final_preds = []\n",
        "    fallback_count = 0\n",
        "\n",
        "    for title, org, ens_pred, ens_conf in zip(titles, organizations, ens_preds, ens_confs):\n",
        "        if ens_conf < confidence_threshold:\n",
        "            kw_pred, kw_conf = keyword_department_classification(title, org)\n",
        "            if kw_conf > ens_conf and kw_pred != 'Other':\n",
        "                final_preds.append(kw_pred)\n",
        "                fallback_count += 1\n",
        "            else:\n",
        "                final_preds.append(ens_pred)\n",
        "        else:\n",
        "            final_preds.append(ens_pred)\n",
        "\n",
        "    print(f\"  Department fallback: {fallback_count}/{len(titles)} ({fallback_count/len(titles)*100:.1f}%)\")\n",
        "    return np.array(final_preds), ens_confs, combined_probs\n",
        "\n",
        "print(\"Ensemble functions defined with weights:\")\n",
        "print(\"  Seniority: MoE=50%, LGB=40%, LR=10%\")\n",
        "print(\"  Department: MoE=30%, LGB=40%, LR=30%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l30-_B4uajQr"
      },
      "source": [
        "---\n",
        "## Section 7: Evaluation\n",
        "\n",
        "### Purpose\n",
        "Evaluate the final ensemble on the test set. We measure:\n",
        "- **Accuracy**: Overall correct predictions\n",
        "- **Balanced Accuracy**: Average per-class recall (handles imbalance)\n",
        "- **F1 Score**: Harmonic mean of precision and recall\n",
        "- **Confusion Matrix**: Detailed error patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1ww7tkWajQr"
      },
      "source": [
        "### 7.1 Prepare Test Features\n",
        "\n",
        "Extract features for test data using the same process as training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsAnQaFmajQs"
      },
      "outputs": [],
      "source": [
        "test_title_features = [extract_title_features(row['position']) for _, row in df_all_test.iterrows()]\n",
        "test_career_features = [extract_career_features(row.get('inactive_jobs', [])) for _, row in df_all_test.iterrows()]\n",
        "\n",
        "test_features_df = pd.DataFrame(test_title_features)\n",
        "career_df = pd.DataFrame(test_career_features)\n",
        "for col in career_df.columns:\n",
        "    if col in FEATURE_COLS:\n",
        "        test_features_df[col] = career_df[col].values\n",
        "\n",
        "test_feat = prepare_features(test_features_df, FEATURE_COLS)\n",
        "print(f\"Test features prepared: {test_feat.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfFmuswzajQs"
      },
      "source": [
        "### 7.2 Run Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg7jQKh8ajQs"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"FINAL EVALUATION ON TEST SET\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTest set size: {len(df_all_test)} samples\")\n",
        "\n",
        "print(\"\\nPredicting Seniority...\")\n",
        "sen_preds, sen_confs, sen_probs = ensemble_predict_seniority(\n",
        "    df_all_test['position'].tolist(), test_emb, test_feat,\n",
        "    moe_sen, lgb_sen, lr_sen, scaler_sen, sen_encoder)\n",
        "\n",
        "print(\"\\nPredicting Department...\")\n",
        "dept_preds, dept_confs, dept_probs = ensemble_predict_department(\n",
        "    df_all_test['position'].tolist(), df_all_test['organization'].fillna('').tolist(),\n",
        "    test_emb, test_feat, moe_dept, lgb_dept, lr_dept, scaler_dept, dept_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZpSWRrlajQs"
      },
      "source": [
        "### 7.3 Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKo0xkGKajQs"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Balanced Accuracy': balanced_accuracy_score(y_true, y_pred),\n",
        "        'F1 Macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "        'F1 Weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        'Precision Macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "        'Recall Macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "    }\n",
        "\n",
        "sen_metrics = calculate_metrics(df_all_test['seniority'], sen_preds)\n",
        "dept_metrics = calculate_metrics(df_all_test['department'], dept_preds)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"                         RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n{'Metric':<25} {'Seniority':>15} {'Department':>15}\")\n",
        "print(\"-\" * 55)\n",
        "for metric in sen_metrics:\n",
        "    print(f\"{metric:<25} {sen_metrics[metric]*100:>14.2f}% {dept_metrics[metric]*100:>14.2f}%\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BV8RVp6ajQt"
      },
      "source": [
        "### 7.4 Classification Reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j14WP8NDajQt"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SENIORITY CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(df_all_test['seniority'], sen_preds, zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DEPARTMENT CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(df_all_test['department'], dept_preds, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTtR9K9iajQt"
      },
      "source": [
        "### 7.5 Confusion Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyC3XS7RajQt"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "cm_sen = confusion_matrix(df_all_test['seniority'], sen_preds, labels=sen_encoder.classes_)\n",
        "sns.heatmap(cm_sen, annot=True, fmt='d', cmap='Blues', xticklabels=sen_encoder.classes_, yticklabels=sen_encoder.classes_, ax=axes[0])\n",
        "axes[0].set_title(f'Seniority (Accuracy: {sen_metrics[\"Accuracy\"]*100:.1f}%)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "cm_dept = confusion_matrix(df_all_test['department'], dept_preds, labels=dept_encoder.classes_)\n",
        "sns.heatmap(cm_dept, annot=True, fmt='d', cmap='Oranges', xticklabels=dept_encoder.classes_, yticklabels=dept_encoder.classes_, ax=axes[1])\n",
        "axes[1].set_title(f'Department (Accuracy: {dept_metrics[\"Accuracy\"]*100:.1f}%)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Confusion matrices saved to outputs/confusion_matrices.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekInqlk-ajQt"
      },
      "source": [
        "### 7.6 Error Analysis\n",
        "\n",
        "Analyze the most common error patterns to understand model weaknesses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uhe8sjFeajQt"
      },
      "outputs": [],
      "source": [
        "sen_errors = df_all_test[df_all_test['seniority'] != sen_preds].copy()\n",
        "sen_errors['predicted'] = sen_preds[df_all_test['seniority'] != sen_preds]\n",
        "\n",
        "print(f\"Seniority Errors: {len(sen_errors)}/{len(df_all_test)} ({len(sen_errors)/len(df_all_test)*100:.1f}%)\")\n",
        "print(\"\\nMost common error patterns:\")\n",
        "error_patterns = sen_errors.groupby(['seniority', 'predicted']).size().sort_values(ascending=False).head(10)\n",
        "for (actual, pred), count in error_patterns.items():\n",
        "    print(f\"  {actual} → {pred}: {count} cases\")\n",
        "\n",
        "print(\"\\nSample misclassified titles:\")\n",
        "for _, row in sen_errors.sample(min(5, len(sen_errors)), random_state=SEED).iterrows():\n",
        "    print(f\"  '{row['position']}' - Actual: {row['seniority']}, Predicted: {row['predicted']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYhvpKO4ajQu"
      },
      "source": [
        "---\n",
        "## Section 8: Explainability\n",
        "\n",
        "### Purpose\n",
        "Understand what drives model predictions through:\n",
        "1. **Feature Importance**: Which engineered features matter most\n",
        "2. **Concept Similarity**: How well class concepts are separated\n",
        "3. **Prediction Examples**: Step-by-step explanation of specific predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzcZPKT9ajQu"
      },
      "source": [
        "### 8.1 LightGBM Feature Importance\n",
        "\n",
        "Visualize which engineered features are most important for LightGBM predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-hJCYnlajQu"
      },
      "outputs": [],
      "source": [
        "feature_names = FEATURE_COLS + [f'lang_{l}' for l in ['EN', 'DE', 'FR']] + \\\n",
        "                [f'pattern_{p}' for p in ['HEAD_OF_X', 'SENIOR_X', 'JUNIOR_X', 'C_LEVEL', 'DIRECTOR_X', 'X_MANAGER', 'X_ENGINEER', 'X_ANALYST', 'CONSULTANT', 'INTERN', 'OTHER']]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "sen_imp = pd.DataFrame({'feature': feature_names, 'importance': lgb_sen.feature_importances_}).sort_values('importance', ascending=False).head(15)\n",
        "axes[0].barh(sen_imp['feature'], sen_imp['importance'], color='steelblue')\n",
        "axes[0].set_xlabel('Importance')\n",
        "axes[0].set_title('Seniority: Top 15 Features (LightGBM)', fontweight='bold')\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "dept_imp = pd.DataFrame({'feature': feature_names, 'importance': lgb_dept.feature_importances_}).sort_values('importance', ascending=False).head(15)\n",
        "axes[1].barh(dept_imp['feature'], dept_imp['importance'], color='darkorange')\n",
        "axes[1].set_xlabel('Importance')\n",
        "axes[1].set_title('Department: Top 15 Features (LightGBM)', fontweight='bold')\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6Gj9aFNajQu"
      },
      "source": [
        "### 8.2 Concept Similarity Analysis\n",
        "\n",
        "Visualize how similar the learned concept embeddings are to each other. Well-separated concepts lead to better classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIXlTOZRajQu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "sen_sim = cosine_similarity(sen_concept_matrix)\n",
        "sns.heatmap(sen_sim, annot=True, fmt='.2f', cmap='coolwarm', xticklabels=sen_encoder.classes_, yticklabels=sen_encoder.classes_, ax=axes[0], vmin=-1, vmax=1)\n",
        "axes[0].set_title('Seniority Concept Similarities', fontweight='bold')\n",
        "\n",
        "dept_sim = cosine_similarity(dept_concept_matrix)\n",
        "sns.heatmap(dept_sim, annot=True, fmt='.2f', cmap='coolwarm', xticklabels=dept_encoder.classes_, yticklabels=dept_encoder.classes_, ax=axes[1], vmin=-1, vmax=1)\n",
        "axes[1].set_title('Department Concept Similarities', fontweight='bold')\n",
        "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv66mz7QajQu"
      },
      "source": [
        "### 8.3 Prediction Explanations\n",
        "\n",
        "Show how the model processes specific job titles step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH-e4kZnajQv"
      },
      "outputs": [],
      "source": [
        "def explain_prediction(title, organization=\"\"):\n",
        "    features = extract_title_features(title)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Title: '{title}'\")\n",
        "    if organization:\n",
        "        print(f\"Organization: '{organization}'\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    print(f\"\\nNormalized: '{normalize_title(title)}'\")\n",
        "    print(f\"Pattern: {features['title_pattern']}\")\n",
        "    print(f\"Language: {features['language']}\")\n",
        "\n",
        "    signals = [k.replace('has_', '').upper() for k, v in features.items() if k.startswith('has_') and v == 1]\n",
        "    print(f\"\\nKeyword signals detected:\")\n",
        "    for s in signals:\n",
        "        print(f\"  • {s}\")\n",
        "    if not signals:\n",
        "        print(\"  (none)\")\n",
        "\n",
        "    print(f\"\\nKeyword fallback results:\")\n",
        "    print(f\"  Seniority: {keyword_seniority_classification(title)[0]} ({keyword_seniority_classification(title)[1]:.0%})\")\n",
        "    print(f\"  Department: {keyword_department_classification(title, organization)[0]} ({keyword_department_classification(title, organization)[1]:.0%})\")\n",
        "\n",
        "explain_prediction(\"Senior Software Engineer\", \"Google\")\n",
        "explain_prediction(\"Geschäftsführer\")\n",
        "explain_prediction(\"Praktikant Marketing\", \"BMW\")\n",
        "explain_prediction(\"Freelance Designer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RidrQtzajQv"
      },
      "source": [
        "---\n",
        "## Section 9: Interactive Demo\n",
        "\n",
        "Launch a Gradio web interface for testing the classifier interactively. The demo provides a public URL that can be shared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3byBnE7hajQv"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def predict_single(title, organization=\"\"):\n",
        "    if not title or not title.strip():\n",
        "        return \"⚠️ Please enter a job title.\"\n",
        "\n",
        "    emb = embed_model.encode([title])\n",
        "    feat_dict = extract_title_features(title)\n",
        "    feat_dict.update(extract_career_features([]))\n",
        "    feat_df = pd.DataFrame([feat_dict])\n",
        "    feat_arr = prepare_features(feat_df, FEATURE_COLS)\n",
        "\n",
        "    sen_pred, sen_conf, _ = ensemble_predict_seniority([title], emb, feat_arr, moe_sen, lgb_sen, lr_sen, scaler_sen, sen_encoder)\n",
        "    dept_pred, dept_conf, _ = ensemble_predict_department([title], [organization], emb, feat_arr, moe_dept, lgb_dept, lr_dept, scaler_dept, dept_encoder)\n",
        "\n",
        "    return f\"\"\"## Results\n",
        "\n",
        "**Seniority:** {sen_pred[0]} ({sen_conf[0]*100:.0f}% confidence)\n",
        "\n",
        "**Department:** {dept_pred[0]} ({dept_conf[0]*100:.0f}% confidence)\"\"\"\n",
        "\n",
        "examples = [\n",
        "    [\"Senior Software Engineer\", \"Google\"],\n",
        "    [\"CEO\", \"Tech Startup GmbH\"],\n",
        "    [\"Praktikant Marketing\", \"BMW\"],\n",
        "    [\"Head of Product\", \"Amazon\"],\n",
        "    [\"Freelance Designer\", \"\"],\n",
        "]\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=predict_single,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Job Title\", placeholder=\"e.g., Senior Software Engineer\"),\n",
        "        gr.Textbox(label=\"Organization (optional)\", placeholder=\"e.g., Google\")\n",
        "    ],\n",
        "    outputs=gr.Markdown(label=\"Prediction\"),\n",
        "    title=\"🎯 LinkedIn Job Title Classifier\",\n",
        "    description=\"Classify job titles into seniority levels and departments. Supports English, German, and French.\",\n",
        "    examples=examples,\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwJxKNrHajQw"
      },
      "source": [
        "---\n",
        "## Section 10: Save Models\n",
        "\n",
        "Export trained models for deployment or later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJmTXY85ajQw"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = '' if (IN_COLAB and BASE_PATH == '/content') else 'models/'\n",
        "OUTPUT_PATH = '' if (IN_COLAB and BASE_PATH == '/content') else 'outputs/'\n",
        "\n",
        "torch.save(moe_sen.state_dict(), f'{MODEL_PATH}moe_seniority.pt')\n",
        "torch.save(moe_dept.state_dict(), f'{MODEL_PATH}moe_department.pt')\n",
        "\n",
        "with open(f'{MODEL_PATH}models_bundle.pkl', 'wb') as f:\n",
        "    pickle.dump({...}, f)\n",
        "\n",
        "\n",
        "test_out.to_csv(f'{OUTPUT_PATH}test_predictions.csv', index=False)\n",
        "prod_out.to_csv(f'{OUTPUT_PATH}production_predictions.csv', index=False)\n",
        "\n",
        "\n",
        "with open(f'{OUTPUT_PATH}results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"✓ Models saved to: {MODEL_PATH}\")\n",
        "print(f\"✓ Outputs saved to: {OUTPUT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fssdUraBajQw"
      },
      "source": [
        "---\n",
        "## Section 11: Future Improvements\n",
        "\n",
        "This section documents potential improvements for the model and data.\n",
        "\n",
        "### Data Improvements\n",
        "\n",
        "| Issue | Impact | Solution |\n",
        "|-------|--------|----------|\n",
        "| \"Other\" department underrepresented (0.4% train vs 51% test) | Low recall for \"Other\" | Collect more \"Other\" examples or use distribution-aware resampling |\n",
        "| Limited German/French data | Lower accuracy for non-English titles | Add multilingual training via translation augmentation |\n",
        "| Small test classes (Junior: 8 samples) | Unreliable metrics | Collect more diverse test data |\n",
        "| Class imbalance in training | Model bias toward majority classes | Apply SMOTE or class-weighted sampling |\n",
        "\n",
        "### Model Improvements\n",
        "\n",
        "| Improvement | Description | Expected Impact |\n",
        "|-------------|-------------|------------------|\n",
        "| Instruction-tuned embeddings | Use `multilingual-e5-large-instruct` with task-specific prompts | +2-5% accuracy |\n",
        "| Confidence calibration | Apply temperature scaling for better probability estimates | More reliable uncertainty |\n",
        "| Active learning | Identify low-confidence predictions for manual labeling | Better coverage of edge cases |\n",
        "| Cross-validation | Use k-fold CV for robust hyperparameter tuning | More stable results |\n",
        "| Ensemble weight tuning | Optimize weights on validation set | +1-2% accuracy |\n",
        "\n",
        "### Deployment Improvements\n",
        "\n",
        "| Improvement | Description |\n",
        "|-------------|-------------|\n",
        "| Model quantization | Reduce model size with INT8/FP16 for faster inference |\n",
        "| Batch API | Support classifying multiple titles per request |\n",
        "| Caching | Cache embeddings for frequently seen titles |\n",
        "| Monitoring | Track prediction distribution to detect data drift |\n",
        "| A/B testing | Compare model versions in production |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
